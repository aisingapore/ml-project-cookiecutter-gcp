{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"End-to-end Project Template (GCP Stack) \u00b6 Customised for {{cookiecutter.project_name}} . Project Description: {{cookiecutter.description}} This template (which also serves as a guide) was generated using the following cookiecutter template: https://github.com/aisingapore/ml-project-cookiecutter-gcp This mkdocs site is for serving the contents of the end-to-end guide in a more readable manner, as opposed to plain Markdown views. The contents of this guide have been customised according to the inputs provided upon generation of this repository through the usage of cruft , following instructions detailed here . Inputs provided to cookiecutter / cruft for the generation of this template: project_name : {{cookiecutter.project_name}} description : {{cookiecutter.description}} repo_name : {{cookiecutter.repo_name}} src_package_name : {{cookiecutter.src_package_name}} src_package_name_short : {{cookiecutter.src_package_name_short}} gcp_project_id : {{cookiecutter.gcp_project_id}} gcr_personal_subdir : {{cookiecutter.gcr_personal_subdir}} author_name : {{cookiecutter.author_name}} open_source_license : {{cookiecutter.open_source_license}} There are two separate guides : User Guide : This guide is for the users of the MLOps platforms and toolings. Admin Guide : This guide is for the administrators and provisioners of the infrastructure required for setting up the components of the MLOps platforms and toolings. Overview For User Guide \u00b6 Prerequisites Preface MLOps Components & Platform Kubernetes kubectl Configuration for GKE Persistent Volumes Polyaxon Polyaxon Dashboard Relevant Concepts Secrets & Credentials on Kubernetes Google Container Registry Development Environment Recommended Setup VSCode JupyterLab Using Docker within Polyaxon Services Cloud SDK for Development Environment Virtual Environment Data Storage & Versioning Sample Data Job Orchestration Pipeline Configuration Data Preparation & Preprocessing Model Training Experiment Tracking Container for Experiment Job Hyperparameter Tuning Deployment Model Artifacts Model Serving (FastAPI) Local Server Docker Container Deploy to GKE Batch Inferencing Continuous Integration & Deployment Documentation GitLab Pages Streamlit Overview for Admin Guide \u00b6 Coming soon... Directory Tree \u00b6 {{cookiecutter.repo_name}} \u251c\u2500\u2500 {{cookiecutter.repo_name}}-conda-env.yml \u2502 ^- The `conda` environment file for reproducing \u2502 the project's development environment. \u251c\u2500\u2500 LICENSE <- The license this repository is to be \u2502 respected under. Can be absent due to \u2502 omission upon generation of repository. \u251c\u2500\u2500 README.md <- The top-level README containing the basic \u2502 guide for using the repository. \u251c\u2500\u2500 .gitlab-ci.yml <- YAML file for configuring instructions for \u2502 GitLab CI/CD. \u251c\u2500\u2500 .dockerignore <- File for specifying files or directories \u2502 to be ignored by Docker contexts. \u251c\u2500\u2500 .pylintrc <- Configurations for `pylint`. \u251c\u2500\u2500 .gitignore <- File for specifying files or directories \u2502 to be ignored by Git. \u251c\u2500\u2500 aisg-context <- Folders containing files and assets relevant \u2502 \u2502 for works within the context of AISG's \u2502 \u2502 development environments. \u2502 \u251c\u2500\u2500 code-server <- Directory containing the entry point script \u2502 \u2502 for the VSCode server's Docker image. \u2502 \u251c\u2500\u2500 guide-site <- Files relevant for spinning up the `mkdocs` \u2502 \u2502 site to view the end-to-end guide. \u2502 \u251c\u2500\u2500 jupyter <- Directory containing the entry point scripts \u2502 \u2502 and config for the Jupyter server's Docker \u2502 \u2502 image. \u2502 \u251c\u2500\u2500 k8s <- Manifest files for spinning up Kubernetes \u2502 \u2502 resources. \u2502 \u2514\u2500\u2500 polyaxon <- Specification files for services and jobs \u2502 to be executed by the Polyaxon server. \u251c\u2500\u2500 assets <- Screenshots and images. \u251c\u2500\u2500 conf <- Configuration files associated with the \u2502 various pipelines as well as for logging. \u251c\u2500\u2500 data <- Folder to contain any data for the various \u2502 pipelines. Ignored by Git except its \u2502 `.gitkeep` file. \u251c\u2500\u2500 docker <- Dockerfiles associated with the various \u2502 stages of the pipeline. \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org \u2502 for details. \u251c\u2500\u2500 models <- Directory for trained and serialised models. \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a \u2502 number (for ordering), the creator's \u2502 initials, and a short `-` delimited \u2502 description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u251c\u2500\u2500 scripts <- Bash scripts for any parts of the pipelines. \u2514\u2500\u2500 src <- Directory containing the source code and | packages for the project repository. |\u2500\u2500 {{cookiecutter.src_package_name}} | ^- Package containing modules for all pipelines | except deployment. |\u2500\u2500 {{cookiecutter.src_package_name}}_fastapi | ^- Package for deploying the predictive models | within a FastAPI server. \u2514\u2500\u2500 tests <- Directory containing tests for the repository's packages. Reference(s): Dockerfile reference - .dockerignore Atlassian's Tutorial on .gitignore GitLab CI/CD Quickstart pylint Docs - Command-line Arguments and Configuration Files","title":"Home"},{"location":"#end-to-end-project-template-gcp-stack","text":"Customised for {{cookiecutter.project_name}} . Project Description: {{cookiecutter.description}} This template (which also serves as a guide) was generated using the following cookiecutter template: https://github.com/aisingapore/ml-project-cookiecutter-gcp This mkdocs site is for serving the contents of the end-to-end guide in a more readable manner, as opposed to plain Markdown views. The contents of this guide have been customised according to the inputs provided upon generation of this repository through the usage of cruft , following instructions detailed here . Inputs provided to cookiecutter / cruft for the generation of this template: project_name : {{cookiecutter.project_name}} description : {{cookiecutter.description}} repo_name : {{cookiecutter.repo_name}} src_package_name : {{cookiecutter.src_package_name}} src_package_name_short : {{cookiecutter.src_package_name_short}} gcp_project_id : {{cookiecutter.gcp_project_id}} gcr_personal_subdir : {{cookiecutter.gcr_personal_subdir}} author_name : {{cookiecutter.author_name}} open_source_license : {{cookiecutter.open_source_license}} There are two separate guides : User Guide : This guide is for the users of the MLOps platforms and toolings. Admin Guide : This guide is for the administrators and provisioners of the infrastructure required for setting up the components of the MLOps platforms and toolings.","title":"End-to-end Project Template (GCP Stack)"},{"location":"#overview-for-user-guide","text":"Prerequisites Preface MLOps Components & Platform Kubernetes kubectl Configuration for GKE Persistent Volumes Polyaxon Polyaxon Dashboard Relevant Concepts Secrets & Credentials on Kubernetes Google Container Registry Development Environment Recommended Setup VSCode JupyterLab Using Docker within Polyaxon Services Cloud SDK for Development Environment Virtual Environment Data Storage & Versioning Sample Data Job Orchestration Pipeline Configuration Data Preparation & Preprocessing Model Training Experiment Tracking Container for Experiment Job Hyperparameter Tuning Deployment Model Artifacts Model Serving (FastAPI) Local Server Docker Container Deploy to GKE Batch Inferencing Continuous Integration & Deployment Documentation GitLab Pages Streamlit","title":"Overview For User Guide"},{"location":"#overview-for-admin-guide","text":"Coming soon...","title":"Overview for Admin Guide"},{"location":"#directory-tree","text":"{{cookiecutter.repo_name}} \u251c\u2500\u2500 {{cookiecutter.repo_name}}-conda-env.yml \u2502 ^- The `conda` environment file for reproducing \u2502 the project's development environment. \u251c\u2500\u2500 LICENSE <- The license this repository is to be \u2502 respected under. Can be absent due to \u2502 omission upon generation of repository. \u251c\u2500\u2500 README.md <- The top-level README containing the basic \u2502 guide for using the repository. \u251c\u2500\u2500 .gitlab-ci.yml <- YAML file for configuring instructions for \u2502 GitLab CI/CD. \u251c\u2500\u2500 .dockerignore <- File for specifying files or directories \u2502 to be ignored by Docker contexts. \u251c\u2500\u2500 .pylintrc <- Configurations for `pylint`. \u251c\u2500\u2500 .gitignore <- File for specifying files or directories \u2502 to be ignored by Git. \u251c\u2500\u2500 aisg-context <- Folders containing files and assets relevant \u2502 \u2502 for works within the context of AISG's \u2502 \u2502 development environments. \u2502 \u251c\u2500\u2500 code-server <- Directory containing the entry point script \u2502 \u2502 for the VSCode server's Docker image. \u2502 \u251c\u2500\u2500 guide-site <- Files relevant for spinning up the `mkdocs` \u2502 \u2502 site to view the end-to-end guide. \u2502 \u251c\u2500\u2500 jupyter <- Directory containing the entry point scripts \u2502 \u2502 and config for the Jupyter server's Docker \u2502 \u2502 image. \u2502 \u251c\u2500\u2500 k8s <- Manifest files for spinning up Kubernetes \u2502 \u2502 resources. \u2502 \u2514\u2500\u2500 polyaxon <- Specification files for services and jobs \u2502 to be executed by the Polyaxon server. \u251c\u2500\u2500 assets <- Screenshots and images. \u251c\u2500\u2500 conf <- Configuration files associated with the \u2502 various pipelines as well as for logging. \u251c\u2500\u2500 data <- Folder to contain any data for the various \u2502 pipelines. Ignored by Git except its \u2502 `.gitkeep` file. \u251c\u2500\u2500 docker <- Dockerfiles associated with the various \u2502 stages of the pipeline. \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org \u2502 for details. \u251c\u2500\u2500 models <- Directory for trained and serialised models. \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a \u2502 number (for ordering), the creator's \u2502 initials, and a short `-` delimited \u2502 description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u251c\u2500\u2500 scripts <- Bash scripts for any parts of the pipelines. \u2514\u2500\u2500 src <- Directory containing the source code and | packages for the project repository. |\u2500\u2500 {{cookiecutter.src_package_name}} | ^- Package containing modules for all pipelines | except deployment. |\u2500\u2500 {{cookiecutter.src_package_name}}_fastapi | ^- Package for deploying the predictive models | within a FastAPI server. \u2514\u2500\u2500 tests <- Directory containing tests for the repository's packages. Reference(s): Dockerfile reference - .dockerignore Atlassian's Tutorial on .gitignore GitLab CI/CD Quickstart pylint Docs - Command-line Arguments and Configuration Files","title":"Directory Tree"},{"location":"guide-for-admin/01-prerequisites/","text":"Prerequisites \u00b6 Coming soon...","title":"Prerequisites"},{"location":"guide-for-admin/01-prerequisites/#prerequisites","text":"Coming soon...","title":"Prerequisites"},{"location":"guide-for-user/01-prerequisites/","text":"Prerequisites \u00b6 Software & Tooling Prerequisites \u00b6 Aside from an internet connection, you would need the following to follow through with the guide: NUS Staff/Student account Google account with @aisingapore.org / @aiap.sg domains provisioned by AI Singapore PC with the following installed: If your machine is with a Windows OS, use PowerShell instead of the default Command ( cmd.exe ) shell. Best if you resort to Windows Terminal . Pulse Secure Refer to NUS IT eGuides for installation guides. Web browser Terminal Git Docker Engine : Client-server application for containerising applications as well as interacting with the Docker daemon. miniconda (recommended) or Anaconda : Virtual environment manager. The former is the minimal installer. gcloud : CLI for interacting with GCP resources. kubectl : CLI for Kubernetes. (Optional) helm : CLI for Kubernetes' package manager. Access to a project on Google Cloud Platform . See here for more information on this. Info Wherever relevant, you can toggle between the different commands that need to be executed for either Linux/macOS or the Windows environment (PowerShell). See below for an example: Linux/macOS # Get a list of files/folders in current directory $ ls -la Windows PowerShell # Get a list of files/folders in current directory $ Get-ChildItem . -Force Caution If you are on Windows OS, you would need to ensure that the files you've cloned or written on your machine be with LF line endings. Otherwise, issues would arise when Docker containers are being built or run. See here on how to configure consistent line endings for a whole folder or workspace using VSCode. NUS VPN \u00b6 Your credentials for your NUS Staff/Student account is needed to login to NUS' VPN for access to the following: AI Singapore's GitLab instance NUS resources In order to interact with remote Git repositories situated on AI Singapore's GitLab instance (clone, push, fetch, etc.) outside of NUS' network or GCP (for regions asia-southeast1 and us-central1 ), you would need to login to NUS' VPN. GitLab \u00b6 We at AI Singapore host our own GitLab server . You should be provided with a set of credentials during onboarding for access to the server. If you would like to configure SSH access for it, you can add the following lines to your SSH config file: Host gitlab.aisingapore.net Port 2222 IdentityFile ~/.ssh/<path_to_key_file> Reference(s): GitLab Docs - Using SSH keys with GitLab CI/CD","title":"Prerequisites"},{"location":"guide-for-user/01-prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"guide-for-user/01-prerequisites/#software-tooling-prerequisites","text":"Aside from an internet connection, you would need the following to follow through with the guide: NUS Staff/Student account Google account with @aisingapore.org / @aiap.sg domains provisioned by AI Singapore PC with the following installed: If your machine is with a Windows OS, use PowerShell instead of the default Command ( cmd.exe ) shell. Best if you resort to Windows Terminal . Pulse Secure Refer to NUS IT eGuides for installation guides. Web browser Terminal Git Docker Engine : Client-server application for containerising applications as well as interacting with the Docker daemon. miniconda (recommended) or Anaconda : Virtual environment manager. The former is the minimal installer. gcloud : CLI for interacting with GCP resources. kubectl : CLI for Kubernetes. (Optional) helm : CLI for Kubernetes' package manager. Access to a project on Google Cloud Platform . See here for more information on this. Info Wherever relevant, you can toggle between the different commands that need to be executed for either Linux/macOS or the Windows environment (PowerShell). See below for an example: Linux/macOS # Get a list of files/folders in current directory $ ls -la Windows PowerShell # Get a list of files/folders in current directory $ Get-ChildItem . -Force Caution If you are on Windows OS, you would need to ensure that the files you've cloned or written on your machine be with LF line endings. Otherwise, issues would arise when Docker containers are being built or run. See here on how to configure consistent line endings for a whole folder or workspace using VSCode.","title":"Software &amp; Tooling Prerequisites"},{"location":"guide-for-user/01-prerequisites/#nus-vpn","text":"Your credentials for your NUS Staff/Student account is needed to login to NUS' VPN for access to the following: AI Singapore's GitLab instance NUS resources In order to interact with remote Git repositories situated on AI Singapore's GitLab instance (clone, push, fetch, etc.) outside of NUS' network or GCP (for regions asia-southeast1 and us-central1 ), you would need to login to NUS' VPN.","title":"NUS VPN"},{"location":"guide-for-user/01-prerequisites/#gitlab","text":"We at AI Singapore host our own GitLab server . You should be provided with a set of credentials during onboarding for access to the server. If you would like to configure SSH access for it, you can add the following lines to your SSH config file: Host gitlab.aisingapore.net Port 2222 IdentityFile ~/.ssh/<path_to_key_file> Reference(s): GitLab Docs - Using SSH keys with GitLab CI/CD","title":"GitLab"},{"location":"guide-for-user/02-preface/","text":"Preface \u00b6 Repository Setup \u00b6 This repository provides an end-to-end template for AI Singapore's AI engineers to onboard their AI projects. Instructions for generating this template is detailed in the cookiecutter template's repository's README.md . While this repository provides users with a set of boilerplates, here you are also presented with a linear guide on how to use them. The boilerplates are rendered and customised when you generated this repository using cruft . Info You can begin by following along the guide as it brings you through a simple problem statement and once you've grasp what this template has to offer, you can deviate from it as much as you wish and customise it to your needs. Since we will be making use of this repository in multiple environments, ensure that this repository is pushed to a remote . Most probably you will be resorting to AI Singapore's GitLab instance as the remote. Refer to here on creating a blank remote repository (or project in GitLab's term). After creating the remote repository, retrieve the remote URL and push the local repository to remote: $ git init $ git remote add origin <REMOTE_URL> $ git add . $ git config user.email \"<YOUR_AISG_EMAIL>\" $ git config user.name \"<YOUR_NAME>\" $ git commit -m \"Initial commit.\" $ git push -u origin master Guide's Problem Statement \u00b6 For this guide, we will work towards building a predictive model that is able to conduct sentiment classification for movie reviews. The model is then to be deployed through a REST API and used for batch inferencing as well. The raw dataset to be used is obtainable through a GCS bucket; instructions for downloading the data into your development environment are detailed under \"Data Storage & Versioning\" , to be referred to later on. Google Cloud Platform (GCP) Projects \u00b6 Each project in AI Singapore that requires the usage of GCP resources would be provided with a GCP project . Such projects are accessible through the GCP console once you've logged into your AI Singapore Google account. Info Projects are managed and provisioned by AI Singapore's Platforms team. If you'd like to request for a project to be created (or for any other enquiries as well), please contact mlops@aisingapore.org . Authorisation \u00b6 You can use GCP's Cloud SDK to interact with the varying GCP services. When you're using the SDK for the first time, you are to provide authorisation using a user or service account. In AI Singapore's context, unless your use case concerns some automation or CI/CD pipelines, you will probably be using your user account (i.e. Google accounts with AI Singapore domains such as @aisingapore.org or @aiap.sg ). See here for more information on authorising your SDK. A simple command to authorise access: $ gcloud auth login To register gcloud for Docker so you can push to Google Container Registry: $ gcloud auth configure-docker With your user account, you should have access to the following GCP products/services: Kubernetes Engine (GKE) Cloud Storage (GCS) Container Registry (GCR)","title":"Preface"},{"location":"guide-for-user/02-preface/#preface","text":"","title":"Preface"},{"location":"guide-for-user/02-preface/#repository-setup","text":"This repository provides an end-to-end template for AI Singapore's AI engineers to onboard their AI projects. Instructions for generating this template is detailed in the cookiecutter template's repository's README.md . While this repository provides users with a set of boilerplates, here you are also presented with a linear guide on how to use them. The boilerplates are rendered and customised when you generated this repository using cruft . Info You can begin by following along the guide as it brings you through a simple problem statement and once you've grasp what this template has to offer, you can deviate from it as much as you wish and customise it to your needs. Since we will be making use of this repository in multiple environments, ensure that this repository is pushed to a remote . Most probably you will be resorting to AI Singapore's GitLab instance as the remote. Refer to here on creating a blank remote repository (or project in GitLab's term). After creating the remote repository, retrieve the remote URL and push the local repository to remote: $ git init $ git remote add origin <REMOTE_URL> $ git add . $ git config user.email \"<YOUR_AISG_EMAIL>\" $ git config user.name \"<YOUR_NAME>\" $ git commit -m \"Initial commit.\" $ git push -u origin master","title":"Repository Setup"},{"location":"guide-for-user/02-preface/#guides-problem-statement","text":"For this guide, we will work towards building a predictive model that is able to conduct sentiment classification for movie reviews. The model is then to be deployed through a REST API and used for batch inferencing as well. The raw dataset to be used is obtainable through a GCS bucket; instructions for downloading the data into your development environment are detailed under \"Data Storage & Versioning\" , to be referred to later on.","title":"Guide's Problem Statement"},{"location":"guide-for-user/02-preface/#google-cloud-platform-gcp-projects","text":"Each project in AI Singapore that requires the usage of GCP resources would be provided with a GCP project . Such projects are accessible through the GCP console once you've logged into your AI Singapore Google account. Info Projects are managed and provisioned by AI Singapore's Platforms team. If you'd like to request for a project to be created (or for any other enquiries as well), please contact mlops@aisingapore.org .","title":"Google Cloud Platform (GCP) Projects"},{"location":"guide-for-user/02-preface/#authorisation","text":"You can use GCP's Cloud SDK to interact with the varying GCP services. When you're using the SDK for the first time, you are to provide authorisation using a user or service account. In AI Singapore's context, unless your use case concerns some automation or CI/CD pipelines, you will probably be using your user account (i.e. Google accounts with AI Singapore domains such as @aisingapore.org or @aiap.sg ). See here for more information on authorising your SDK. A simple command to authorise access: $ gcloud auth login To register gcloud for Docker so you can push to Google Container Registry: $ gcloud auth configure-docker With your user account, you should have access to the following GCP products/services: Kubernetes Engine (GKE) Cloud Storage (GCS) Container Registry (GCR)","title":"Authorisation"},{"location":"guide-for-user/03-mlops-components-platform/","text":"MLOps Components & Platform \u00b6 Components Flowchart \u00b6 The images below showcase the different components that this guide will cover as well as how each of them relate to each other. Info The diagrams follow the C4 model for visualising and describing software architecture. Context View \u00b6 Containers View \u00b6 Kubernetes \u00b6 We will be using Kubernetes as the underlying orchestration tool to execute pipelines and manage containerised applications and environments. From the Kubernetes site: Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. A number of services and applications that you will be interacting with (or deploying) are deployed (to be deployed) within a GKE cluster environment. A GKE cluster should be set up upon creation of your GCP project, viewable here . If this is not the case, please notify the MLOps team at mlops@aisingapore.org . Some of the MLOps components which the GKE environment will be relevant for are: Developer Workspace Model Experimentation Experiment Tracking Data/Artifact Storage Model Serving What this means is that AI engineers would need to be able to access the GKE cluster. Documentation for obtaining a cluster's configuration can be found here . kubectl Configuration for GKE \u00b6 If you would like to view or create Kubernetes (GKE) resources within development environments (or even on your own local machine), you can run the following command to connect to the GKE cluster which by default your user or service account should have access to: $ gcloud container clusters get-credentials <NAME_OF_CLUSTER> --zone asia-southeast1-c --project {{ cookiecutter.gcp_project_id }} After obtaining the credentials and configurations for the GKE cluster, you can start to interact with the main MLOps platforms tool that you will be leveraging on for a development workspace, data preparation as well as model training. Caution Non-staff engineer accounts or service accounts are granted limited permissions. One would not be able to carry out certain actions with the clusters such as viewing namespaces or deleting cluster resources. Reference(s): GKE Overview Persistent Volumes \u00b6 Containers are ephemeral and what that translates to is that any data created and stored within the containers' file systems as well as any changes made to it will be gone once the container is stopped. To persist data or files, we would need to mount volumes to the containers. With the default configuration provided in this template, any services or jobs to be spun up on the MLOps platform Polyaxon will have a persistent volume attached. The volume's mount path is /polyaxon-v1-data and so anything that is stored within that path will be persisted. Reference(s): Kubernetes Docs - Volumes NetApp - What are Kubernetes persistent volumes? Polyaxon \u00b6 Polyaxon is an MLOps platform that provides a suite of features for AI engineers to facilitate their end-to-end machine learning workflows. The platform is to be deployed on a GKE cluster; the Platforms team would have set the platform up for your team upon creation of the GCP project. AI engineers need not worry about having to administer the platform as end-consumers of the platform. To verify if Polyaxon has been deployed on your GKE cluster, you can run the following command: $ helm list --namespace polyaxon-v1 NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION polyaxon polyaxon-v1 X 2021 -XX-XX XX:XX:XX +0800 +08 deployed polyaxon-1.11.3 1 .11.3 An output similar to the one above should be returned. Polyaxon Dashboard \u00b6 Now, let's access the dashboard for Polyaxon. Before we can interact with the platform, we have to install the Polyaxon CLI. This should ideally be done within a virtual Python environment. You can conveniently get the relevant libraries for this guide by executing the following command within the repository: $ conda env create -f {{ cookiecutter.repo_name }} -conda-env.yml At any point of time you would like to interact with the Polyaxon server, you would need port forwarding of the Polyaxon Kubernetes service to your localhost . You can do port forwarding to a port on the localhost with the Polyaxon CLI (we'll go ahead with the port 8117 ): $ polyaxon port-forward --port = 8117 --namespace = polyaxon-v1 Tip The above process would occupy your current terminal. If you'd like the process to run in the background instead of opening up another terminal window/tab, you can run the following commands: Linux/macOS $ polyaxon port-forward --port = 8117 --namespace = polyaxon-v1 & > /dev/null 2 > & 1 & Windows PowerShell $ Start-Process polyaxon -ArgumentList \"port-forward --port=8117 --namespace=polyaxon-v1\" However, for Linux/macOS, to end these processes that are running in the background, you cannot simply abort the process by using Ctrl + C . Do look at this resource on how to kill running (background) processes. Open up a browser and head over to localhost:8117 . You should see an interface as such: Before we can create any services or run jobs on the platform, we have to configure the host for the CLI and create a project within the platform: $ polyaxon config set --host = http://localhost:8117 $ polyaxon project create --name {{ cookiecutter.repo_name }} -<YOUR_NAME> After the command above, you should see a project with the name you've specified above in the projects dashboard . Reference(s): Ampersands & on the command line Relevant Concepts \u00b6 Before we proceed further, let's cover some basic concepts that one should know when getting started with Polyaxon. Polyaxonfiles: To submit jobs or spin up services on the Polyaxon platform, users would have to make use of both the CLI as well as Polyaxon-specific config files known as Polyaxonfiles. The CLI establishes communications and connections with the Polyaxon server while Polyaxonfiles provide specification to the server for the kind of request you are making. Polyaxonfiles can be written and defined in several formats (YAML, JSON, Python, and some other languages) but in AI Singapore's context, we will be sticking with YAML. Polyaxonfile boilerplates can be found under aisg-context/polyaxon/polyaxonfiles . Head over here for the official documentation on Polyaxonfiles. Components: Before you can define a job or service for Polyaxon, you would have to call upon and define a component. Hence, in every Polyaxonfile that is provided as an example, you see the following lines at the very beginning: version : 1.1 kind : component ... From the official documentation: Component is a discrete, repeatable, and self-contained action that defines an environment and a runtime. Essentially, a \"component\" is to represent a discrete aspect of your end-to-end workflow. You can have one component for your development environment, one for your data preparation pipeline, and another for your model training workflow. You can also have different components for different variations of your pipelines. However, for these workflows to be defined, we have to start with specifying a component. Shown here , you can specify various runtimes (experimentation tools) you would like to spin up on the Polyaxon server. Reference(s): Polyaxon Docs - Component Specification Polyaxon Docs - Polyaxon experimentation tools Jobs: One example of such components is \"jobs\". You can run jobs for training models, data preprocessing or any generic tasks that are executable within Docker containers. Whenever you need to execute a pipeline or a one-off task, \"jobs\" is the right runtime to go for. Reference(s): Polyaxon Docs - Jobs Introduction Services: The \"services\" runtime is used for spinning up applications or interfaces that are to remain running until you stop the service (or an error is faced on the server's end). You can spin up services the likes of a VSCode editor that would be accessible via a browser, a Jupyter Lab server or a REST API server. Reference(s): Polyaxon Docs - Services Introduction We will dive deeper into these concepts and the usage of each one of them in later sections. Secrets & Credentials on Kubernetes \u00b6 When executing jobs on Polyaxon, credentials are needed to access various services like GCR or GCS. To provide your container jobs with access to these credentials, you need to carry out the following: Download a service account key to your local machine (or obtain it from the lead engineer/MLOps team) and rename it to gcp-service-account.json . Take note of the client email detailed in the JSON file. The client email should look something like the following: <SA_CLIENT_EMAIL_FROM_SA_KEY>@{{cookiecutter.gcp_project_id}}.iam.gserviceaccount.com . Create a Kubernetes secret on your Kubernetes (GKE) cluster, within the same namespace where Polyaxon is deployed: polyaxon-v1 . Configure Polyaxonfiles to refer to these secrets. Before creating the secrets, do check if they already exist first: Linux/macOS $ kubectl get secret --namespace = polyaxon-v1 | grep -E 'gcp-imagepullsecrets|gcp-sa-credentials' Windows PowerShell $ kubectl get secret --namespace = polyaxon-v1 | Select-String \"gcp-imagepullsecrets\" $ kubectl get secret --namespace = polyaxon-v1 | Select-String \"gcp-sa-credentials\" Here are the commands to be executed for creating the secrets: Linux/macOS $ export SA_CLIENT_EMAIL = <SA_CLIENT_EMAIL_FROM_SA_KEY> $ export PATH_TO_SA_JSON_FILE = <PATH_TO_SA_JSON_FILE> $ kubectl create secret docker-registry gcp-imagepullsecrets \\ --docker-server = https://asia.gcr.io \\ --docker-username = _json_key \\ --docker-email = $SA_CLIENT_EMAIL \\ --docker-password = \" $( cat $PATH_TO_SA_JSON_FILE ) \" \\ --namespace = polyaxon-v1 $ kubectl create secret generic gcp-sa-credentials \\ --from-file $PATH_TO_SA_JSON_FILE \\ --namespace = polyaxon-v1 Windows PowerShell $ $SA_CLIENT_EMAIL =< SA_CLIENT_EMAIL_FROM_SA_KEY > $ $PATH_TO_SA_JSON_FILE =< PATH_TO_SA_JSON_FILE > $ kubectl create secret docker-registry gcp-imagepullsecrets ` - -docker-server = https :// asia . gcr . io ` - -docker-username = _json_key ` - -docker-email = $SA_CLIENT_EMAIL ` - -docker-password = '$(cat $PATH_TO_SA_JSON_FILE)' ` - -namespace = polyaxon-v1 $ kubectl create secret generic gcp-sa-credentials ` - -from -file $PATH_TO_SA_JSON_FILE ` - -namespace = polyaxon-v1 Make sure that the Polyaxonfiles for the jobs and services that requires your service account credentials have the following configurations (these snippets are included in the rendered Polyaxonfiles by default): ... inputs : - name : SA_CRED_PATH description : Path to credential file for GCP service account. isOptional : true type : str value : /var/secret/cloud.google.com/gcp-service-account.json toEnv : GOOGLE_APPLICATION_CREDENTIALS run : kind : job environment : imagePullSecrets : [ \"gcp-imagepullsecrets\" ] volumes : - name : gcp-service-account secret : secretName : \"gcp-sa-credentials\" container : volumeMounts : - name : gcp-service-account mountPath : /var/secret/cloud.google.com ... By configuring and specifying the secrets in your Polyaxonfiles like the above, you would be able to utilise the gcloud or gsutil commands within your containerised jobs/services. Here are some examples: use private Docker images from GCR to spin up jobs/services on Polyaxon uploading model artifacts to GCS buckets within model training jobs interact with GKE cluster(s) within Polyaxon jobs/services Reference(s): Kubernetes Docs - Namespaces Google Container Registry \u00b6 AI Singapore's emphases on reproducibility and portability of workflows and accompanying environments translates to heavy usage of containerisation. Throughout this guide, we will be building Docker images necessary for setting up development environments, jobs for the various pipelines and deployment of the predictive model. Within the context of GCP, the Google Container Registry (GCR) will be used to store and version our Docker images. Following authorisation to gcloud , you can view the image repositories of your project's registry like so: {% if cookiecutter.gcr_personal_subdir == 'No' %} $ gcloud container images list --repository = asia.gcr.io/ {{ cookiecutter.gcp_project_id }} {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} $ gcloud container images list --repository = asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} {% endif %} You will be pushing the Docker images to the aforementioned repository. Reference(s): gcloud Reference - gcloud container images list GCR Guide - Pushing & Pulling Images","title":"MLOps Components & Platform"},{"location":"guide-for-user/03-mlops-components-platform/#mlops-components-platform","text":"","title":"MLOps Components &amp; Platform"},{"location":"guide-for-user/03-mlops-components-platform/#components-flowchart","text":"The images below showcase the different components that this guide will cover as well as how each of them relate to each other. Info The diagrams follow the C4 model for visualising and describing software architecture.","title":"Components Flowchart"},{"location":"guide-for-user/03-mlops-components-platform/#context-view","text":"","title":"Context View"},{"location":"guide-for-user/03-mlops-components-platform/#containers-view","text":"","title":"Containers View"},{"location":"guide-for-user/03-mlops-components-platform/#kubernetes","text":"We will be using Kubernetes as the underlying orchestration tool to execute pipelines and manage containerised applications and environments. From the Kubernetes site: Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. A number of services and applications that you will be interacting with (or deploying) are deployed (to be deployed) within a GKE cluster environment. A GKE cluster should be set up upon creation of your GCP project, viewable here . If this is not the case, please notify the MLOps team at mlops@aisingapore.org . Some of the MLOps components which the GKE environment will be relevant for are: Developer Workspace Model Experimentation Experiment Tracking Data/Artifact Storage Model Serving What this means is that AI engineers would need to be able to access the GKE cluster. Documentation for obtaining a cluster's configuration can be found here .","title":"Kubernetes"},{"location":"guide-for-user/03-mlops-components-platform/#kubectl-configuration-for-gke","text":"If you would like to view or create Kubernetes (GKE) resources within development environments (or even on your own local machine), you can run the following command to connect to the GKE cluster which by default your user or service account should have access to: $ gcloud container clusters get-credentials <NAME_OF_CLUSTER> --zone asia-southeast1-c --project {{ cookiecutter.gcp_project_id }} After obtaining the credentials and configurations for the GKE cluster, you can start to interact with the main MLOps platforms tool that you will be leveraging on for a development workspace, data preparation as well as model training. Caution Non-staff engineer accounts or service accounts are granted limited permissions. One would not be able to carry out certain actions with the clusters such as viewing namespaces or deleting cluster resources. Reference(s): GKE Overview","title":"kubectl Configuration for GKE"},{"location":"guide-for-user/03-mlops-components-platform/#persistent-volumes","text":"Containers are ephemeral and what that translates to is that any data created and stored within the containers' file systems as well as any changes made to it will be gone once the container is stopped. To persist data or files, we would need to mount volumes to the containers. With the default configuration provided in this template, any services or jobs to be spun up on the MLOps platform Polyaxon will have a persistent volume attached. The volume's mount path is /polyaxon-v1-data and so anything that is stored within that path will be persisted. Reference(s): Kubernetes Docs - Volumes NetApp - What are Kubernetes persistent volumes?","title":"Persistent Volumes"},{"location":"guide-for-user/03-mlops-components-platform/#polyaxon","text":"Polyaxon is an MLOps platform that provides a suite of features for AI engineers to facilitate their end-to-end machine learning workflows. The platform is to be deployed on a GKE cluster; the Platforms team would have set the platform up for your team upon creation of the GCP project. AI engineers need not worry about having to administer the platform as end-consumers of the platform. To verify if Polyaxon has been deployed on your GKE cluster, you can run the following command: $ helm list --namespace polyaxon-v1 NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION polyaxon polyaxon-v1 X 2021 -XX-XX XX:XX:XX +0800 +08 deployed polyaxon-1.11.3 1 .11.3 An output similar to the one above should be returned.","title":"Polyaxon"},{"location":"guide-for-user/03-mlops-components-platform/#polyaxon-dashboard","text":"Now, let's access the dashboard for Polyaxon. Before we can interact with the platform, we have to install the Polyaxon CLI. This should ideally be done within a virtual Python environment. You can conveniently get the relevant libraries for this guide by executing the following command within the repository: $ conda env create -f {{ cookiecutter.repo_name }} -conda-env.yml At any point of time you would like to interact with the Polyaxon server, you would need port forwarding of the Polyaxon Kubernetes service to your localhost . You can do port forwarding to a port on the localhost with the Polyaxon CLI (we'll go ahead with the port 8117 ): $ polyaxon port-forward --port = 8117 --namespace = polyaxon-v1 Tip The above process would occupy your current terminal. If you'd like the process to run in the background instead of opening up another terminal window/tab, you can run the following commands: Linux/macOS $ polyaxon port-forward --port = 8117 --namespace = polyaxon-v1 & > /dev/null 2 > & 1 & Windows PowerShell $ Start-Process polyaxon -ArgumentList \"port-forward --port=8117 --namespace=polyaxon-v1\" However, for Linux/macOS, to end these processes that are running in the background, you cannot simply abort the process by using Ctrl + C . Do look at this resource on how to kill running (background) processes. Open up a browser and head over to localhost:8117 . You should see an interface as such: Before we can create any services or run jobs on the platform, we have to configure the host for the CLI and create a project within the platform: $ polyaxon config set --host = http://localhost:8117 $ polyaxon project create --name {{ cookiecutter.repo_name }} -<YOUR_NAME> After the command above, you should see a project with the name you've specified above in the projects dashboard . Reference(s): Ampersands & on the command line","title":"Polyaxon Dashboard"},{"location":"guide-for-user/03-mlops-components-platform/#relevant-concepts","text":"Before we proceed further, let's cover some basic concepts that one should know when getting started with Polyaxon. Polyaxonfiles: To submit jobs or spin up services on the Polyaxon platform, users would have to make use of both the CLI as well as Polyaxon-specific config files known as Polyaxonfiles. The CLI establishes communications and connections with the Polyaxon server while Polyaxonfiles provide specification to the server for the kind of request you are making. Polyaxonfiles can be written and defined in several formats (YAML, JSON, Python, and some other languages) but in AI Singapore's context, we will be sticking with YAML. Polyaxonfile boilerplates can be found under aisg-context/polyaxon/polyaxonfiles . Head over here for the official documentation on Polyaxonfiles. Components: Before you can define a job or service for Polyaxon, you would have to call upon and define a component. Hence, in every Polyaxonfile that is provided as an example, you see the following lines at the very beginning: version : 1.1 kind : component ... From the official documentation: Component is a discrete, repeatable, and self-contained action that defines an environment and a runtime. Essentially, a \"component\" is to represent a discrete aspect of your end-to-end workflow. You can have one component for your development environment, one for your data preparation pipeline, and another for your model training workflow. You can also have different components for different variations of your pipelines. However, for these workflows to be defined, we have to start with specifying a component. Shown here , you can specify various runtimes (experimentation tools) you would like to spin up on the Polyaxon server. Reference(s): Polyaxon Docs - Component Specification Polyaxon Docs - Polyaxon experimentation tools Jobs: One example of such components is \"jobs\". You can run jobs for training models, data preprocessing or any generic tasks that are executable within Docker containers. Whenever you need to execute a pipeline or a one-off task, \"jobs\" is the right runtime to go for. Reference(s): Polyaxon Docs - Jobs Introduction Services: The \"services\" runtime is used for spinning up applications or interfaces that are to remain running until you stop the service (or an error is faced on the server's end). You can spin up services the likes of a VSCode editor that would be accessible via a browser, a Jupyter Lab server or a REST API server. Reference(s): Polyaxon Docs - Services Introduction We will dive deeper into these concepts and the usage of each one of them in later sections.","title":"Relevant Concepts"},{"location":"guide-for-user/03-mlops-components-platform/#secrets-credentials-on-kubernetes","text":"When executing jobs on Polyaxon, credentials are needed to access various services like GCR or GCS. To provide your container jobs with access to these credentials, you need to carry out the following: Download a service account key to your local machine (or obtain it from the lead engineer/MLOps team) and rename it to gcp-service-account.json . Take note of the client email detailed in the JSON file. The client email should look something like the following: <SA_CLIENT_EMAIL_FROM_SA_KEY>@{{cookiecutter.gcp_project_id}}.iam.gserviceaccount.com . Create a Kubernetes secret on your Kubernetes (GKE) cluster, within the same namespace where Polyaxon is deployed: polyaxon-v1 . Configure Polyaxonfiles to refer to these secrets. Before creating the secrets, do check if they already exist first: Linux/macOS $ kubectl get secret --namespace = polyaxon-v1 | grep -E 'gcp-imagepullsecrets|gcp-sa-credentials' Windows PowerShell $ kubectl get secret --namespace = polyaxon-v1 | Select-String \"gcp-imagepullsecrets\" $ kubectl get secret --namespace = polyaxon-v1 | Select-String \"gcp-sa-credentials\" Here are the commands to be executed for creating the secrets: Linux/macOS $ export SA_CLIENT_EMAIL = <SA_CLIENT_EMAIL_FROM_SA_KEY> $ export PATH_TO_SA_JSON_FILE = <PATH_TO_SA_JSON_FILE> $ kubectl create secret docker-registry gcp-imagepullsecrets \\ --docker-server = https://asia.gcr.io \\ --docker-username = _json_key \\ --docker-email = $SA_CLIENT_EMAIL \\ --docker-password = \" $( cat $PATH_TO_SA_JSON_FILE ) \" \\ --namespace = polyaxon-v1 $ kubectl create secret generic gcp-sa-credentials \\ --from-file $PATH_TO_SA_JSON_FILE \\ --namespace = polyaxon-v1 Windows PowerShell $ $SA_CLIENT_EMAIL =< SA_CLIENT_EMAIL_FROM_SA_KEY > $ $PATH_TO_SA_JSON_FILE =< PATH_TO_SA_JSON_FILE > $ kubectl create secret docker-registry gcp-imagepullsecrets ` - -docker-server = https :// asia . gcr . io ` - -docker-username = _json_key ` - -docker-email = $SA_CLIENT_EMAIL ` - -docker-password = '$(cat $PATH_TO_SA_JSON_FILE)' ` - -namespace = polyaxon-v1 $ kubectl create secret generic gcp-sa-credentials ` - -from -file $PATH_TO_SA_JSON_FILE ` - -namespace = polyaxon-v1 Make sure that the Polyaxonfiles for the jobs and services that requires your service account credentials have the following configurations (these snippets are included in the rendered Polyaxonfiles by default): ... inputs : - name : SA_CRED_PATH description : Path to credential file for GCP service account. isOptional : true type : str value : /var/secret/cloud.google.com/gcp-service-account.json toEnv : GOOGLE_APPLICATION_CREDENTIALS run : kind : job environment : imagePullSecrets : [ \"gcp-imagepullsecrets\" ] volumes : - name : gcp-service-account secret : secretName : \"gcp-sa-credentials\" container : volumeMounts : - name : gcp-service-account mountPath : /var/secret/cloud.google.com ... By configuring and specifying the secrets in your Polyaxonfiles like the above, you would be able to utilise the gcloud or gsutil commands within your containerised jobs/services. Here are some examples: use private Docker images from GCR to spin up jobs/services on Polyaxon uploading model artifacts to GCS buckets within model training jobs interact with GKE cluster(s) within Polyaxon jobs/services Reference(s): Kubernetes Docs - Namespaces","title":"Secrets &amp; Credentials on Kubernetes"},{"location":"guide-for-user/03-mlops-components-platform/#google-container-registry","text":"AI Singapore's emphases on reproducibility and portability of workflows and accompanying environments translates to heavy usage of containerisation. Throughout this guide, we will be building Docker images necessary for setting up development environments, jobs for the various pipelines and deployment of the predictive model. Within the context of GCP, the Google Container Registry (GCR) will be used to store and version our Docker images. Following authorisation to gcloud , you can view the image repositories of your project's registry like so: {% if cookiecutter.gcr_personal_subdir == 'No' %} $ gcloud container images list --repository = asia.gcr.io/ {{ cookiecutter.gcp_project_id }} {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} $ gcloud container images list --repository = asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} {% endif %} You will be pushing the Docker images to the aforementioned repository. Reference(s): gcloud Reference - gcloud container images list GCR Guide - Pushing & Pulling Images","title":"Google Container Registry"},{"location":"guide-for-user/04-dev-env/","text":"Development Environment \u00b6 An advantage presented by the Polyaxon platform is that you can utilise the GKE cluster's resources for your development and engineering works instead of your own resources. We can make use of Polyaxon services to spin up VSCode or JupyterLab servers with which cluster resources can be dedicated (except for GPUs); all you need on your end is a machine with WebSockets, a browser, and a terminal. Recommended Setup \u00b6 While there exist the option for engineers to set up either a VSCode or JupyterLab service (or both), the former would suffice. Reason being VSCode is an excellent code editor with integrated terminal capabilities and it can also work with Jupyter notebooks. The JupyterLab on the other hand, while being the best interface for Jupyter notebooks, has subpar UX for its terminal and code editor. That is to be expected however as it is the dedicated environment for everything related to the Jupyter ecosystem. Attention First user of each GCP project's Polyaxon server is to begin with a VSCode service as it is required in setting up the folder for dedicated workspaces through an integrated terminal. VSCode \u00b6 The VSCode service to be created will be using a Docker image. You can use the Dockerfile that is provided out-of-the-box docker/{{cookiecutter.repo_name}}-poly-vscode.Dockerfile to build a Docker image to be pushed to your project's container registry (GCR) or you can customise that same Dockerfile to your liking. Let's build the image: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-vscode.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / vscode-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-vscode . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / vscode-server : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /vscode-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-vscode.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /vscode-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ vscode-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-vscode . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ vscode-server : 0 . 1 . 0 {% endif %} Push the configurations to the Polyaxon server to start up the VSCode service: Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml ` -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter.repo_name }} -<YOUR_NAME> {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/vscode-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml ` -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/vscode-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter.repo_name }} -<YOUR_NAME> {% endif %} - -P is used to pass an input for a parameter defined in the Polyaxonfile that is in question. In this case, we are specifying to the Polyaxon service that we intend to use the Docker image asia.gcr.io/$GCP_PROJECT_ID/vscode-server:0.1.0 and to work from the path that we have specified, which is /polyaxon-v1-data where data and artifacts will be persisted. - The -p flag is used to specify the project for which you are dedicating the service or job to. Now head over to the services dashboard under your project. The link to your services would be as such - http://localhost:8117/ui/default/{{cookiecutter.repo_name}}-<YOUR_NAME>/services . The interface should look something like the following: To access the VSCode service, expand the service and click on the Service tab: The service you see here is embedded within Polyaxon's dashboard. You can click on the Fullscreen button to have a single browser tab be dedicated to this service. Important Now, open up an integrated terminal within the Polyaxon VSCode environment which can be done using the keyboard shortcut Ctrl + Shift + ` . Persistent Workspaces \u00b6 As mentioned, a persistent volume claim is to be attached to the containers in the cluster, accessible through the path /polyaxon-v1-data . However, it is initially empty. What is to be done now is for a subdirectory named workspaces to be created and be owned by the user ID 2222 , a figure observable in most of the default boilerplate Dockerfiles. Do a quick check if the directory /polyaxon-v1-data/workspaces exists: Polyaxon VSCode Terminal $ ls -la /polyaxon-v1-data | grep \"workspaces\" Attention If workspaces does not exist Polyaxon VSCode Terminal $ sudo mkdir -p /polyaxon-v1-data/workspaces/<YOUR_NAME> $ sudo chown -R 2222 :2222 /polyaxon-v1-data/workspaces $ cd /polyaxon-v1-data/workspaces/<YOUR_NAME> If workspaces does exist Polyaxon VSCode Terminal $ mkdir /polyaxon-v1-data/workspaces/<YOUR_NAME> && cd \" $_ \" Use this subdirectory as your own personal workspace, where all your work and other relevant assets can be persisted. Git from VSCode \u00b6 To clone or push to Git repositories within the VSCode integrated terminal, it is recommended that you first disable VSCode's Git authentication handler: Head over to File > Preferences > Settings . Search for git.terminalAuthentication . Uncheck the option. Open a new integrated terminal. We can now clone this repository (or any other repository) into the environment's persistent storage. As the persistent storage would be accessible by the rest of your project team members, you should only use the HTTPS protocol to clone the repository as opposed to using an SSH key. The path to persistent storage on Polyaxon is located at /polyaxon-v1-data and you can create your own workspace folder under /polyaxon-v1-data/workspaces/<YOUR_NAME> . Now, let's clone your repository from the remote: Polyaxon VSCode Terminal $ cd /polyaxon-v1-data/workspaces/<YOUR_NAME> $ git clone <REMOTE_URL_HTTPS> $ cd {{ cookiecutter.repo_name }} Extensions for VSCode \u00b6 You can install a multitude of extensions for your VSCode service but there are a couple that would be crucial for your workflow, especially if you intend to use Jupyter notebooks within the VSCode environment. ms-python.python : Official extension by Microsoft for rich support for many things Python. ms-toolsai.jupyter : Official extension by Microsoft for Jupyter support. Info Do head over here on how to enable the usage of virtual conda environments within VSCode. JupyterLab \u00b6 Attention Setting up of a JupyterLab server is optional and not needed as a VSCode service is sufficient as a developer workspace. Resources are limited so use only what you need. While Jupyter Notebooks are viewable, editable and executable within a VSCode environment, most are still more familiar with Jupyter's interface for interacting with or editing notebooks. We can spin up a JupyterLab service on Polyaxon: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /jupyter-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-jupyter.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /jupyter-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / jupyter-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-jupyter . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / jupyter-server : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /jupyter-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-jupyter.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /jupyter-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ jupyter-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-jupyter . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ jupyter-server : 0 . 1 . 0 {% endif %} Push the configurations to the Polyaxon server to start up the Jupyter service: Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/jupyter-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /jupyter-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / jupyter-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/jupyter-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/jupyter-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/jupyter-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / jupyter-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/jupyter-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} Now head over to the services dashboard under your project. The service should be accompanied with the tags jupyter , notebook and lab . Info Do head over here on how to enable the usage of virtual conda environments within JupyterLab. Using Docker within Polyaxon Services \u00b6 Caution Since these development environments are essentially pods deployed within a Kubernetes cluster, using Docker within the pods themselves is not feasible by default and while possible, should be avoided. Cloud SDK for Development Environment \u00b6 Attention Through the default boilerplate Dockerfiles, the Google Cloud SDK is installed only for VSCode among the development environment services. As mentioned here , credentials or secrets can be attached to Polyaxon services or jobs when configured properly. In doing so, you can make use of Google service accounts to interact with GCP services or resources. You can configure the gcloud CLI to make use of the service account credentials attached to the services or jobs, which its path is set to the environment variable GOOGLE_APPLICATION_CREDENTIALS : $ gcloud auth activate-service-account <SA_CLIENT_EMAIL_FROM_SA_KEY>@ {{ cookiecutter.gcp_project_id }} .iam.gserviceaccount.com --key-file = $GOOGLE_APPLICATION_CREDENTIALS Once the service account has been configured, examples of actions you can carry out consists of the following: list objects within GCS buckets create objects within GCS buckets list deployed pods within a GKE cluster The service account is granted a custom role with specific permissions deemed needed by AI engineers. Reference(s): Polyaxon Docs - Run CLI Reference Polyaxon - Integrations Cloud SDK Reference - gcloud auth activate-service-account Using Docker-in-Docker for your CI or testing environment? Think twice. - jpetazzo","title":"Development Environment"},{"location":"guide-for-user/04-dev-env/#development-environment","text":"An advantage presented by the Polyaxon platform is that you can utilise the GKE cluster's resources for your development and engineering works instead of your own resources. We can make use of Polyaxon services to spin up VSCode or JupyterLab servers with which cluster resources can be dedicated (except for GPUs); all you need on your end is a machine with WebSockets, a browser, and a terminal.","title":"Development Environment"},{"location":"guide-for-user/04-dev-env/#recommended-setup","text":"While there exist the option for engineers to set up either a VSCode or JupyterLab service (or both), the former would suffice. Reason being VSCode is an excellent code editor with integrated terminal capabilities and it can also work with Jupyter notebooks. The JupyterLab on the other hand, while being the best interface for Jupyter notebooks, has subpar UX for its terminal and code editor. That is to be expected however as it is the dedicated environment for everything related to the Jupyter ecosystem. Attention First user of each GCP project's Polyaxon server is to begin with a VSCode service as it is required in setting up the folder for dedicated workspaces through an integrated terminal.","title":"Recommended Setup"},{"location":"guide-for-user/04-dev-env/#vscode","text":"The VSCode service to be created will be using a Docker image. You can use the Dockerfile that is provided out-of-the-box docker/{{cookiecutter.repo_name}}-poly-vscode.Dockerfile to build a Docker image to be pushed to your project's container registry (GCR) or you can customise that same Dockerfile to your liking. Let's build the image: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-vscode.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / vscode-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-vscode . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / vscode-server : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /vscode-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-vscode.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /vscode-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ vscode-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-vscode . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ vscode-server : 0 . 1 . 0 {% endif %} Push the configurations to the Polyaxon server to start up the VSCode service: Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml ` -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /vscode-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter.repo_name }} -<YOUR_NAME> {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/vscode-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/vscode-service.yml ` -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/vscode-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter.repo_name }} -<YOUR_NAME> {% endif %} - -P is used to pass an input for a parameter defined in the Polyaxonfile that is in question. In this case, we are specifying to the Polyaxon service that we intend to use the Docker image asia.gcr.io/$GCP_PROJECT_ID/vscode-server:0.1.0 and to work from the path that we have specified, which is /polyaxon-v1-data where data and artifacts will be persisted. - The -p flag is used to specify the project for which you are dedicating the service or job to. Now head over to the services dashboard under your project. The link to your services would be as such - http://localhost:8117/ui/default/{{cookiecutter.repo_name}}-<YOUR_NAME>/services . The interface should look something like the following: To access the VSCode service, expand the service and click on the Service tab: The service you see here is embedded within Polyaxon's dashboard. You can click on the Fullscreen button to have a single browser tab be dedicated to this service. Important Now, open up an integrated terminal within the Polyaxon VSCode environment which can be done using the keyboard shortcut Ctrl + Shift + ` .","title":"VSCode"},{"location":"guide-for-user/04-dev-env/#persistent-workspaces","text":"As mentioned, a persistent volume claim is to be attached to the containers in the cluster, accessible through the path /polyaxon-v1-data . However, it is initially empty. What is to be done now is for a subdirectory named workspaces to be created and be owned by the user ID 2222 , a figure observable in most of the default boilerplate Dockerfiles. Do a quick check if the directory /polyaxon-v1-data/workspaces exists: Polyaxon VSCode Terminal $ ls -la /polyaxon-v1-data | grep \"workspaces\" Attention If workspaces does not exist Polyaxon VSCode Terminal $ sudo mkdir -p /polyaxon-v1-data/workspaces/<YOUR_NAME> $ sudo chown -R 2222 :2222 /polyaxon-v1-data/workspaces $ cd /polyaxon-v1-data/workspaces/<YOUR_NAME> If workspaces does exist Polyaxon VSCode Terminal $ mkdir /polyaxon-v1-data/workspaces/<YOUR_NAME> && cd \" $_ \" Use this subdirectory as your own personal workspace, where all your work and other relevant assets can be persisted.","title":"Persistent Workspaces"},{"location":"guide-for-user/04-dev-env/#git-from-vscode","text":"To clone or push to Git repositories within the VSCode integrated terminal, it is recommended that you first disable VSCode's Git authentication handler: Head over to File > Preferences > Settings . Search for git.terminalAuthentication . Uncheck the option. Open a new integrated terminal. We can now clone this repository (or any other repository) into the environment's persistent storage. As the persistent storage would be accessible by the rest of your project team members, you should only use the HTTPS protocol to clone the repository as opposed to using an SSH key. The path to persistent storage on Polyaxon is located at /polyaxon-v1-data and you can create your own workspace folder under /polyaxon-v1-data/workspaces/<YOUR_NAME> . Now, let's clone your repository from the remote: Polyaxon VSCode Terminal $ cd /polyaxon-v1-data/workspaces/<YOUR_NAME> $ git clone <REMOTE_URL_HTTPS> $ cd {{ cookiecutter.repo_name }}","title":"Git from VSCode"},{"location":"guide-for-user/04-dev-env/#extensions-for-vscode","text":"You can install a multitude of extensions for your VSCode service but there are a couple that would be crucial for your workflow, especially if you intend to use Jupyter notebooks within the VSCode environment. ms-python.python : Official extension by Microsoft for rich support for many things Python. ms-toolsai.jupyter : Official extension by Microsoft for Jupyter support. Info Do head over here on how to enable the usage of virtual conda environments within VSCode.","title":"Extensions for VSCode"},{"location":"guide-for-user/04-dev-env/#jupyterlab","text":"Attention Setting up of a JupyterLab server is optional and not needed as a VSCode service is sufficient as a developer workspace. Resources are limited so use only what you need. While Jupyter Notebooks are viewable, editable and executable within a VSCode environment, most are still more familiar with Jupyter's interface for interacting with or editing notebooks. We can spin up a JupyterLab service on Polyaxon: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /jupyter-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-jupyter.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /jupyter-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / jupyter-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-jupyter . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / jupyter-server : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /jupyter-server:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -poly-jupyter.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /jupyter-server:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ jupyter-server : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -poly-jupyter . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ jupyter-server : 0 . 1 . 0 {% endif %} Push the configurations to the Polyaxon server to start up the Jupyter service: Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/jupyter-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /jupyter-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / jupyter-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/jupyter-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/jupyter-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/jupyter-server:0.1.0\" \\ -P WORKING_DIR = \"/polyaxon-v1-data\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / jupyter-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/jupyter-server:0.1.0\" ` -P WORKING_DIR = \"/polyaxon-v1-data\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} Now head over to the services dashboard under your project. The service should be accompanied with the tags jupyter , notebook and lab . Info Do head over here on how to enable the usage of virtual conda environments within JupyterLab.","title":"JupyterLab"},{"location":"guide-for-user/04-dev-env/#using-docker-within-polyaxon-services","text":"Caution Since these development environments are essentially pods deployed within a Kubernetes cluster, using Docker within the pods themselves is not feasible by default and while possible, should be avoided.","title":"Using Docker within Polyaxon Services"},{"location":"guide-for-user/04-dev-env/#cloud-sdk-for-development-environment","text":"Attention Through the default boilerplate Dockerfiles, the Google Cloud SDK is installed only for VSCode among the development environment services. As mentioned here , credentials or secrets can be attached to Polyaxon services or jobs when configured properly. In doing so, you can make use of Google service accounts to interact with GCP services or resources. You can configure the gcloud CLI to make use of the service account credentials attached to the services or jobs, which its path is set to the environment variable GOOGLE_APPLICATION_CREDENTIALS : $ gcloud auth activate-service-account <SA_CLIENT_EMAIL_FROM_SA_KEY>@ {{ cookiecutter.gcp_project_id }} .iam.gserviceaccount.com --key-file = $GOOGLE_APPLICATION_CREDENTIALS Once the service account has been configured, examples of actions you can carry out consists of the following: list objects within GCS buckets create objects within GCS buckets list deployed pods within a GKE cluster The service account is granted a custom role with specific permissions deemed needed by AI engineers. Reference(s): Polyaxon Docs - Run CLI Reference Polyaxon - Integrations Cloud SDK Reference - gcloud auth activate-service-account Using Docker-in-Docker for your CI or testing environment? Think twice. - jpetazzo","title":"Cloud SDK for Development Environment"},{"location":"guide-for-user/05-virtual-env/","text":"Virtual Environment \u00b6 While the Docker images you will be using to run experiments on Polyaxon would contain the conda environments you would need, you can also create these virtual environments within your development environment, and have it be persisted. The following set of commands allows you to create the conda environment and store the packages within your own workspace directory: First, have VSCode open the repository that you have cloned previously by heading over to the top left hand corner, selecting File > Open Folder... , and entering the path to the repository. In this case, you should be navigating to the folder /polyaxon-v1-data/workspaces/<YOUR_NAME>/{{cookiecutter.repo_name}} . Now, let's initialise conda for the bash shell, and create the virtual environment specified in {{cookiecutter.repo_name}}-conda-env.yml . Polyaxon VSCode Terminal $ /miniconda3/bin/conda init bash $ source ~/.bashrc ( base ) $ conda env create -f {{ cookiecutter.repo_name }} -conda-env.yml \\ -p /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/ {{ cookiecutter.repo_name }} -conda-env After creating the conda environment, let's create a permanent alias for easy activation. Polyaxon VSCode Terminal ( base ) $ echo 'alias {{cookiecutter.repo_name}}-conda-env=\"conda activate /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/{{cookiecutter.repo_name}}-conda-env\"' >> ~/.bashrc ( base ) $ source ~/.bashrc ( base ) $ {{ cookiecutter.repo_name }} -conda-env ({{ cookiecutter.repo_name }} -conda-env ) $ # conda environment has been activated Tip If you encounter issues in trying to install Python libraries, do ensure that the amount of resources allocated to the VSCode service is sufficient. Installation of libraries from PyPI tends to fail when there's insufficient memory. For starters, dedicate 4GB of memory to the service: ... resources : requests : memory : \"4Gi\" cpu : \"2.5\" limits : memory : \"4Gi\" cpu : \"2.5\" ... Another way is to add the flag --no-cache-dir for your pip install executions. However, there's no similar flag for conda at the moment so the above is a blanket solution. Reference(s): conda Docs - Managing environments StackOverflow - \"Pip install killed - out of memory - how to get around it?\" phoenixNAP - Linux alias Command: How to Use It With Examples Jupyter Kernel for VSCode \u00b6 While it is possible for VSCode to make use of different virtual Python environments, some other additional steps are required for Polyaxon VSCode service to detect the conda environments that you would have created. Ensure that you are in a project folder which you intend to work on. You can open a folder through File > Open Folder... . In this case, you should be navigating to the folder /polyaxon-v1-data/workspaces/<YOUR_NAME/{{cookiecutter.repo_name}} . Install the VSCode extensions ms-python.python and ms-toolsai.jupyter . After installation of these extensions, restart VSCode by using the shortcut Ctrl + Shift + P , entering Developer: Reload Window in the prompt and pressing Enter following that. Ensure that you have ipykernel installed in the conda environment that you intend to use. This template by default lists the library as a dependency under {{cookiecutter.repo_name}}-conda-env.yml . You can check for the library like so: Polyaxon VSCode Terminal $ conda activate /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/ {{ cookiecutter.repo_name }} -conda-env $ conda list | grep \"ipykernel\" ipykernel 6 .9.2 pypi_0 pypi Now enter Ctrl + Shift + P again and execute Python: Select Interpreter . Provide the path to the Python executable within the conda environment that you intend to use, something like so: path/to/conda_env/bin/python . Open up any Jupyter notebook and click on the button that says Select Kernel on the top right hand corner. You will be presented with a selection of Python interpreters. Select the one that corresponds to the environment you intend to use. Test out the kernel by running the cells in the sample notebook provided under notebooks/sample-tf-classification.ipynb . Jupyter Kernel for JupyterLab \u00b6 The same with the VSCode service, the JupyterLab service would not by default detect conda environments. You would have to specify to the JupyterLab installation the ipython kernel existing within your conda environment. Open up a terminal within JupyterLab . Activate the conda environment in question and ensure that you have ipykernel installed in the conda environment that you intend to use. This template by default lists the library as a dependency under {{cookiecutter.repo_name}}-conda-env.yml . You can check for the library like so: JupyterLab Terminal $ conda activate /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/ {{ cookiecutter.repo_name }} -conda-env $ conda list | grep \"ipykernel\" ipykernel 6 .9.2 pypi_0 pypi Within the conda environment, execute the following: JupyterLab Terminal $ ipython kernel install --name \"{{cookiecutter.repo_name}}-conda-env\" --user Refresh the JupyterLab instance. Within each Jupyter notebook, you can select the kernel of specific conda environments that you intend to use by heading to the toolbar under Kernel -> Change Kernel... . Test out the kernel by running the cells in the sample notebook provided under notebooks/sample-tf-classification.ipynb . Reference(s): Jupyter Docs - Kernels (Programming Languages)","title":"Virtual Environment"},{"location":"guide-for-user/05-virtual-env/#virtual-environment","text":"While the Docker images you will be using to run experiments on Polyaxon would contain the conda environments you would need, you can also create these virtual environments within your development environment, and have it be persisted. The following set of commands allows you to create the conda environment and store the packages within your own workspace directory: First, have VSCode open the repository that you have cloned previously by heading over to the top left hand corner, selecting File > Open Folder... , and entering the path to the repository. In this case, you should be navigating to the folder /polyaxon-v1-data/workspaces/<YOUR_NAME>/{{cookiecutter.repo_name}} . Now, let's initialise conda for the bash shell, and create the virtual environment specified in {{cookiecutter.repo_name}}-conda-env.yml . Polyaxon VSCode Terminal $ /miniconda3/bin/conda init bash $ source ~/.bashrc ( base ) $ conda env create -f {{ cookiecutter.repo_name }} -conda-env.yml \\ -p /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/ {{ cookiecutter.repo_name }} -conda-env After creating the conda environment, let's create a permanent alias for easy activation. Polyaxon VSCode Terminal ( base ) $ echo 'alias {{cookiecutter.repo_name}}-conda-env=\"conda activate /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/{{cookiecutter.repo_name}}-conda-env\"' >> ~/.bashrc ( base ) $ source ~/.bashrc ( base ) $ {{ cookiecutter.repo_name }} -conda-env ({{ cookiecutter.repo_name }} -conda-env ) $ # conda environment has been activated Tip If you encounter issues in trying to install Python libraries, do ensure that the amount of resources allocated to the VSCode service is sufficient. Installation of libraries from PyPI tends to fail when there's insufficient memory. For starters, dedicate 4GB of memory to the service: ... resources : requests : memory : \"4Gi\" cpu : \"2.5\" limits : memory : \"4Gi\" cpu : \"2.5\" ... Another way is to add the flag --no-cache-dir for your pip install executions. However, there's no similar flag for conda at the moment so the above is a blanket solution. Reference(s): conda Docs - Managing environments StackOverflow - \"Pip install killed - out of memory - how to get around it?\" phoenixNAP - Linux alias Command: How to Use It With Examples","title":"Virtual Environment"},{"location":"guide-for-user/05-virtual-env/#jupyter-kernel-for-vscode","text":"While it is possible for VSCode to make use of different virtual Python environments, some other additional steps are required for Polyaxon VSCode service to detect the conda environments that you would have created. Ensure that you are in a project folder which you intend to work on. You can open a folder through File > Open Folder... . In this case, you should be navigating to the folder /polyaxon-v1-data/workspaces/<YOUR_NAME/{{cookiecutter.repo_name}} . Install the VSCode extensions ms-python.python and ms-toolsai.jupyter . After installation of these extensions, restart VSCode by using the shortcut Ctrl + Shift + P , entering Developer: Reload Window in the prompt and pressing Enter following that. Ensure that you have ipykernel installed in the conda environment that you intend to use. This template by default lists the library as a dependency under {{cookiecutter.repo_name}}-conda-env.yml . You can check for the library like so: Polyaxon VSCode Terminal $ conda activate /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/ {{ cookiecutter.repo_name }} -conda-env $ conda list | grep \"ipykernel\" ipykernel 6 .9.2 pypi_0 pypi Now enter Ctrl + Shift + P again and execute Python: Select Interpreter . Provide the path to the Python executable within the conda environment that you intend to use, something like so: path/to/conda_env/bin/python . Open up any Jupyter notebook and click on the button that says Select Kernel on the top right hand corner. You will be presented with a selection of Python interpreters. Select the one that corresponds to the environment you intend to use. Test out the kernel by running the cells in the sample notebook provided under notebooks/sample-tf-classification.ipynb .","title":"Jupyter Kernel for VSCode"},{"location":"guide-for-user/05-virtual-env/#jupyter-kernel-for-jupyterlab","text":"The same with the VSCode service, the JupyterLab service would not by default detect conda environments. You would have to specify to the JupyterLab installation the ipython kernel existing within your conda environment. Open up a terminal within JupyterLab . Activate the conda environment in question and ensure that you have ipykernel installed in the conda environment that you intend to use. This template by default lists the library as a dependency under {{cookiecutter.repo_name}}-conda-env.yml . You can check for the library like so: JupyterLab Terminal $ conda activate /polyaxon-v1-data/workspaces/<YOUR_NAME>/conda_envs/ {{ cookiecutter.repo_name }} -conda-env $ conda list | grep \"ipykernel\" ipykernel 6 .9.2 pypi_0 pypi Within the conda environment, execute the following: JupyterLab Terminal $ ipython kernel install --name \"{{cookiecutter.repo_name}}-conda-env\" --user Refresh the JupyterLab instance. Within each Jupyter notebook, you can select the kernel of specific conda environments that you intend to use by heading to the toolbar under Kernel -> Change Kernel... . Test out the kernel by running the cells in the sample notebook provided under notebooks/sample-tf-classification.ipynb . Reference(s): Jupyter Docs - Kernels (Programming Languages)","title":"Jupyter Kernel for JupyterLab"},{"location":"guide-for-user/06-data-storage-versioning/","text":"Data Storage & Versioning \u00b6 So we now have our development environment. However, the data for us to conduct EDA and predictive modeling on is nowhere in sight. Since the context of this guide is within the Google Cloud Platform, we will be making use of its Google Cloud Storage service for remote and object storage. In most cases, raw data for 100E projects provided by the project sponsors are uploaded to GCS through AI Singapore's in-house tool for uploading data: UDP. When data is uploaded to GCS through UDP, a GCS bucket is populated with timestamped directories containing whatever raw that was uploaded. For example , assuming your GCP project ID is ai-proj-aut0 , a GCS bucket with a similar name like ai-proj-aut0 should exist. The tree in the bucket can look something like this: . \u2514\u2500\u2500 ai-proj-aut0 \u251c\u2500\u2500 20211214_1109568/ \u251c\u2500\u2500 20211214_1110821/ \u251c\u2500\u2500 20211214_1448200/ \u2514\u2500\u2500 20211214_1449172/ The subdirectories are timestamps indicative of the time that the raw data was added to GCS. A question that follows is how do we get the data into the persistent storage that's accessible by Polyaxon's jobs and services? Well, let's go back to the VSCode environment to use the integrated terminal. From this section , we've learned how to set up authorisation for gcloud to use the service account credentials we have attached to the service containers. This means that we can use gsutil to pull in data from a GCS bucket. Polyaxon VSCode Terminal $ mkdir -p /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/ai-proj-aut0 && cd \" $_ \" $ gsutil -m rsync -r gs://ai-proj-aut0 . The -m flag is to utilise parallel synchronisation which would speed things up and the -r concerns recursion through a bucket/directory. Now, when a new set of raw data has been uploaded, a new subdirectory will appear in the bucket. Say the tree now looks like the following: . \u2514\u2500\u2500 ai-proj-aut0 \u251c\u2500\u2500 20211215_0952332/ \u251c\u2500\u2500 20211214_1109568/ \u251c\u2500\u2500 20211214_1110821/ \u251c\u2500\u2500 20211214_1448200/ \u2514\u2500\u2500 20211214_1449172/ To retrieve that new set of raw data i.e. the folder 20211215_0952332 , just run the same command in the relevant folder again: Polyaxon VSCode Terminal $ cd /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/ai-proj-aut0 $ gsutil -m rsync -r gs://ai-proj-aut0 . The gsutil utility will synchronise the directory in the persistent storage with the remote object storage/bucket. With that said, do not place your processed data directory within this same directory containing all the raw data lest you'd lose the processed data upon sychronisation. Let the directory for raw data contain just raw data. Processed/Intermediary Data \u00b6 As mentioned, raw data for 100E projects are located in a default GCS bucket within each GCP project, usually with the convention gs://<GCP_PROJECT_ID>-aut0 . However, that default bucket should only contain raw data . Each project should create a separate GCS bucket for processed or any other form of data. Say one would like to create a GCS bucket with the name {{cookiecutter.repo_name}}-proc-data , the following command can be used: Local Machine / Polyaxon VSCode Terminal $ gsutil mb -p {{ cookiecutter.gcp_project_id }} -c STANDARD -l ASIA-SOUTHEAST1 -b on gs:// {{ cookiecutter.repo_name }} -proc-data Creating gs:// {{ cookiecutter.repo_name }} -proc-data/... Note Something to take note for the future is that GCS buckets are to adhere to the naming guidelines set by GCP, one of which is that bucket names must be globally unique. See here for more information on naming guidelines for GCS buckets. DO look into the naming conventions for GCS objects here as well. Sample Data \u00b6 While you may have your own project data to work with, for the purpose of following through with this template guide, let's download the sample data for the sample problem statement at hand. Info The sample data for this guide's problem statement is made accessible to the public. Hence any team or individual can download it. It is highly likely that your project's data is not publicly accessible and neither should it be, especially if it is a 100E project. Polyaxon VSCode Terminal $ mkdir -p /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg && cd \" $_ \" $ gsutil -m rsync -r gs://acl-movie-review-data-aisg . In the following section, we will work towards processing the raw data and eventually training a sentiment classifier model. Reference(s): gsutil Reference - rsync","title":"Data Storage & Versioning"},{"location":"guide-for-user/06-data-storage-versioning/#data-storage-versioning","text":"So we now have our development environment. However, the data for us to conduct EDA and predictive modeling on is nowhere in sight. Since the context of this guide is within the Google Cloud Platform, we will be making use of its Google Cloud Storage service for remote and object storage. In most cases, raw data for 100E projects provided by the project sponsors are uploaded to GCS through AI Singapore's in-house tool for uploading data: UDP. When data is uploaded to GCS through UDP, a GCS bucket is populated with timestamped directories containing whatever raw that was uploaded. For example , assuming your GCP project ID is ai-proj-aut0 , a GCS bucket with a similar name like ai-proj-aut0 should exist. The tree in the bucket can look something like this: . \u2514\u2500\u2500 ai-proj-aut0 \u251c\u2500\u2500 20211214_1109568/ \u251c\u2500\u2500 20211214_1110821/ \u251c\u2500\u2500 20211214_1448200/ \u2514\u2500\u2500 20211214_1449172/ The subdirectories are timestamps indicative of the time that the raw data was added to GCS. A question that follows is how do we get the data into the persistent storage that's accessible by Polyaxon's jobs and services? Well, let's go back to the VSCode environment to use the integrated terminal. From this section , we've learned how to set up authorisation for gcloud to use the service account credentials we have attached to the service containers. This means that we can use gsutil to pull in data from a GCS bucket. Polyaxon VSCode Terminal $ mkdir -p /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/ai-proj-aut0 && cd \" $_ \" $ gsutil -m rsync -r gs://ai-proj-aut0 . The -m flag is to utilise parallel synchronisation which would speed things up and the -r concerns recursion through a bucket/directory. Now, when a new set of raw data has been uploaded, a new subdirectory will appear in the bucket. Say the tree now looks like the following: . \u2514\u2500\u2500 ai-proj-aut0 \u251c\u2500\u2500 20211215_0952332/ \u251c\u2500\u2500 20211214_1109568/ \u251c\u2500\u2500 20211214_1110821/ \u251c\u2500\u2500 20211214_1448200/ \u2514\u2500\u2500 20211214_1449172/ To retrieve that new set of raw data i.e. the folder 20211215_0952332 , just run the same command in the relevant folder again: Polyaxon VSCode Terminal $ cd /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/ai-proj-aut0 $ gsutil -m rsync -r gs://ai-proj-aut0 . The gsutil utility will synchronise the directory in the persistent storage with the remote object storage/bucket. With that said, do not place your processed data directory within this same directory containing all the raw data lest you'd lose the processed data upon sychronisation. Let the directory for raw data contain just raw data.","title":"Data Storage &amp; Versioning"},{"location":"guide-for-user/06-data-storage-versioning/#processedintermediary-data","text":"As mentioned, raw data for 100E projects are located in a default GCS bucket within each GCP project, usually with the convention gs://<GCP_PROJECT_ID>-aut0 . However, that default bucket should only contain raw data . Each project should create a separate GCS bucket for processed or any other form of data. Say one would like to create a GCS bucket with the name {{cookiecutter.repo_name}}-proc-data , the following command can be used: Local Machine / Polyaxon VSCode Terminal $ gsutil mb -p {{ cookiecutter.gcp_project_id }} -c STANDARD -l ASIA-SOUTHEAST1 -b on gs:// {{ cookiecutter.repo_name }} -proc-data Creating gs:// {{ cookiecutter.repo_name }} -proc-data/... Note Something to take note for the future is that GCS buckets are to adhere to the naming guidelines set by GCP, one of which is that bucket names must be globally unique. See here for more information on naming guidelines for GCS buckets. DO look into the naming conventions for GCS objects here as well.","title":"Processed/Intermediary Data"},{"location":"guide-for-user/06-data-storage-versioning/#sample-data","text":"While you may have your own project data to work with, for the purpose of following through with this template guide, let's download the sample data for the sample problem statement at hand. Info The sample data for this guide's problem statement is made accessible to the public. Hence any team or individual can download it. It is highly likely that your project's data is not publicly accessible and neither should it be, especially if it is a 100E project. Polyaxon VSCode Terminal $ mkdir -p /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg && cd \" $_ \" $ gsutil -m rsync -r gs://acl-movie-review-data-aisg . In the following section, we will work towards processing the raw data and eventually training a sentiment classifier model. Reference(s): gsutil Reference - rsync","title":"Sample Data"},{"location":"guide-for-user/07-job-orchestration/","text":"Job Orchestration \u00b6 Even though we can set up development workspaces to execute jobs and workflows, these environments often have limited access to resources. To carry out heavier workloads, we encourage the usage of job orchestration features that the Polyaxon platform has. Jobs are submitted to the Polyaxon server and executed within Docker containers. These images are either pulled from a registry or built upon a job's submission. The names and definitions of images are specified in Polyaxonfiles. Using these images, Kubernetes pods are spun up to execute the entry points or commands defined, tapping on to the Kubernetes cluster's available resources. Any jobs that are submitted to the Polyaxon server can be tracked and monitored through Polyaxon's dashboard. See this section on how to access the dashboard for Polyaxon and create a project. Pipeline Configuration \u00b6 In this template, Hydra is the configuration framework of choice for the data preparation and model training pipelines (or any pipelines that doesn't belong to the model serving aspects). The configurations for logging, pipelines and hyperparameter tuning can be found under conf/base . These YAML files are then referred to by Hydra or general utility functions ( src/{{cookiecutter.src_package_name}}/general_utils.py ) for loading of parameters and configurations. The defined default values can be overridden through the CLI. Attention It is recommended that you have a basic understanding of Hydra 's concepts before you move on. Reference(s): Hydra Docs - Basic Override Syntax Data Preparation & Preprocessing \u00b6 To process the sample raw data, we will be spinning up a job on Polyaxon. This job will be using a Docker image that will be built from a Dockerfile ( docker/{{cookiecutter.repo_name}}-data-prep.Dockerfile ) provided in this template: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /data-prep:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -data-prep.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /data-prep:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / data-prep : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -data-prep . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / data-prep : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /data-prep:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -data-prep.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /data-prep:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ data-prep : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -data-prep . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ data-prep : 0 . 1 . 0 {% endif %} Assuming you're still connected to the Polyaxon server through port-forwarding, submit a job to the server like such: Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/process-data.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /data-prep:0.1.0\" \\ -P RAW_DATA_DIRS = '[\"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1\"]' \\ -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / process -data . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/data-prep:0.1.0\" ` -P RAW_DATA_DIRS = \"['/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1']\" ` -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/process-data.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/data-prep:0.1.0\" \\ -P RAW_DATA_DIRS = '[\"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1\"]' \\ -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / process -data . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/data-prep:0.1.0\" ` -P RAW_DATA_DIRS = \"['/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1']\" ` -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} Info If you were to inspect aisg-context/polyaxon/polyaxonfiles/process-data.yml , the second command with yq overwrites the list of directories specified in the config file conf/base/pipelines.yml for the key .data_prep.raw_dirs_paths . You may specify a list of directory paths with which you can process and combine the results into one single directory. The yq utility is used to overwrite the values in the YAML config as Hydra currently doesn't support modification of list in YAML files . After some time, the data processing job should conclude and we can proceed with training the predictive model. The processed data is exported to the directory /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined . We will be passing this path to the model training workflows. Model Training \u00b6 Now that we have processed the raw data, we can look into training the sentiment classification model. The script relevant for this section is src/train_model.py . In this script, you can see it using some utility functions from src/{{cookiecutter.src_package_name}}/general_utils.py as well, most notably the functions for utilising MLflow utilities for tracking experiments. Let's set up the tooling for experiment tracking before we start model experimentation. Experiment Tracking \u00b6 In the module src/{{cookiecutter.src_package_name}}/general_utils.py , the functions mlflow_init and mlflow_log are used to initialise MLflow experiments as well as log information and artifacts relevant for a run to a remote MLflow Tracking server. An MLflow Tracking server is usually set up within a GKE cluster for projects that requires model experimentation. Artifacts logged through the MLflow API can be uploaded to GCS buckets, assuming the client is authorised for access to GCS. Note The username and password for the MLflow Tracking server can be retrieved from the MLOps team or your team lead. To log and upload artifacts to GCS buckets through MLflow, you need to do the following first: Ensure that the Polyaxon job for model experimentation is configured to use your GCP project's service account credentials. See \"Secrets & Credentials on Kubernetes\" on how to do this. Create a bucket for storing such artifacts on GCS. Create an MLflow experiment on the tracking server. Let's create a GCS bucket for storing all your model experiment artifacts (assuming the bucket has yet to be created): Important For the purpose of this guide, we will create a bucket with the following name: gs://{{cookiecutter.repo_name}}-artifacts . The Docker images that we will be using to spin up inference servers will download exported model artifacts from this bucket, under a subdirectory mlflow-tracking-server . Hence, the longer path of the directory where all the model artifacts will be stored at will be gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server . Local Machine / Polyaxon VSCode Terminal $ gsutil mb -p {{ cookiecutter.gcp_project_id }} -c STANDARD -l ASIA-SOUTHEAST1 -b on gs:// {{ cookiecutter.repo_name }} -artifacts Creating gs:// {{ cookiecutter.repo_name }} -artifacts/... Now, let's access the MLflow Tracking server's dashboard to create an experiment for us to log runs to. Open a separate terminal and run the following: Local Machine $ kubectl port-forward service/mlflow-nginx-server-svc 5005 :5005 --namespace = polyaxon-v1 Head over to your web browser and access the following URL: http://localhost:5005 . You would be prompted for a username and password and upon a successful entry you should be presented with an interface similar to the one below: We are to collate runs under experiments. See here for the distinction between runs and experiments. For each experiment, we can specify paths and URLs for which we intend to upload artifacts to. To create an experiment, locate a + button on the top left hand corner of the interface. A pop-up box follows prompting a name for the experiment and a location for artifacts to be stored at. {% if cookiecutter.gcr_personal_subdir == 'No' %} For the current use case, let's make use of the following path for location of artifacts: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} For the current use case, let's make use of the path we have specified for MLflow artifacts, with your name appended: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}} . For name of experiment, you can specify a name like so: ml-experiment-{{cookiecutter.author_name}} . Warning Here are some things to take note when creating an experiment: Specifying object storage paths for a new experiment through MLflow's CLI does not work well currently so we would have to make do with creation of experiments through the UI. It is highly recommended that experiment names are without whitespaces . Words can be concatenated with hyphens. Reference(s): MLflow Docs - Tracking MLflow Docs - Tracking (Artifact Stores) Container for Experiment Job \u00b6 Before we submit a job to Polyaxon to train our model, we need to build the Docker image to be used for it: Linux/macOS $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -model-training-gpu.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0 Windows PowerShell $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / model-train : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -model-training-gpu . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / model-train : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /model-train:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -model-training-gpu.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /model-train:0.1.0 Windows PowerShell $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ model-train : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -model-training-gpu . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ model-train : 0 . 1 . 0 {% endif %} Now that we have the Docker image pushed to the registry, we can run a job using it: Attention If the GKE cluster is without GPU nodes, the Polyaxonfiles would have to be edited to remove any request for GPUs (otherwise the job cannot be scheduled). Delete the following lines: ... tolerations : - key : \"nvidia.com/gpu\" operator : \"Equal\" value : \"present\" effect : \"NoSchedule\" ... nvidia.com/gpu : 1 ... {% if cookiecutter.gcr_personal_subdir == 'No' %} Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} !!! caution Do take note of the backtick (`) before the colon in the value for the parameter MLFLOW_TRACKING_URI . See here for some explanation. Hyperparameter Tuning \u00b6 For many ML problems, we would be bothered with finding the optimal parameters to train our models with. While we are able to override the parameters for our model training workflows, imagine having to sweep through a distribution of values. For example, if you were to seek for the optimal learning rate within a log space, we would have to execute polyaxon run a myriad of times manually, just to provide the training script with a different learning rate value each time. It is reasonable that one seeks for ways to automate this workflow. Optuna is an optimisation framework designed for ML use-cases. Its features includes: ease of modularity, optimisation algorithms for searching the best set of parameters, and paralellisation capabilities for faster sweeps. In addition, Hydra has a plugin for utilising Optuna which further translates to ease of configuration. To use Hydra's plugin for Optuna, we have to provide further overrides within the YAML config, and this is observed in conf/base/train-model-hptuning.yml : defaults : - override hydra/sweeper : \"optuna\" - override hydra/sweeper/sampler : \"tpe\" hydra : sweeper : sampler : seed : 123 direction : [ \"minimize\" , \"maximize\" ] study_name : \"sentiment-classification\" storage : null n_trials : 3 n_jobs : 1 search_space : train.val_split : type : \"float\" low : 0.2 high : 0.35 step : 0.025 train.optimiser : type : \"categorical\" choices : [ \"adam\" , \"rmsprop\" ] Attention The fields defined are terminologies used by Optuna. Therefore, it is recommended that you understand the basics of the tool. This overview video covers well on the concepts brought upon by Optuna. The script with which hyperparameter tuning is conducted, src/train_model_hptuning.py , has 2 essential lines that are different from src/train_model.py : ... @hydra . main ( config_path = \"../conf/base\" , config_name = \"train-model-hptuning.yml\" ) ... return test_loss , test_acc The first change specifies the different config file needed for utilising the Optuna plugin. The second one is needed for Optuna to judge the performance of the objectives (i.e. metrics) within each iteration, or as they put it, \"trials\". Another difference with this workflow is that for each trial with a different set of parameters, a new MLflow run has to be initialised. However, we need to somehow link all these different runs together so that we can compare all the runs within a single Optuna study (set of trials). How we do this is that we provide the script with a tag ( hptuning_tag ) which would essentially be the date epoch value of the moment you submitted the job to Polyaxon. This tag is defined using the environment value MLFLOW_HPTUNING_TAG . Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu-hptuning.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P MLFLOW_HPTUNING_TAG = \" $( date +%s ) \" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu-hptuning . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P MLFLOW_HPTUNING_TAG =$( Get-Date -UFormat % s -Millisecond 0 ) ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu-hptuning.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P MLFLOW_HPTUNING_TAG = \" $( date +%s ) \" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu-hptuning . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P MLFLOW_HPTUNING_TAG =$( Get-Date -UFormat % s -Millisecond 0 ) ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} !!! caution Do take note of the backtick (`) before the colon in the value for the parameter MLFLOW_TRACKING_URI . See here for some explanation. Say the tag is 1641159546 , you can then filter this within MLflow's dashboard for runs with the associated tag like such in the search bar: tags.hptuning_tag=\"1641159546\" . Reference(s): Hydra Docs - Optuna Sweeper Plugin MLflow Docs - Search Syntax","title":"Job Orchestration"},{"location":"guide-for-user/07-job-orchestration/#job-orchestration","text":"Even though we can set up development workspaces to execute jobs and workflows, these environments often have limited access to resources. To carry out heavier workloads, we encourage the usage of job orchestration features that the Polyaxon platform has. Jobs are submitted to the Polyaxon server and executed within Docker containers. These images are either pulled from a registry or built upon a job's submission. The names and definitions of images are specified in Polyaxonfiles. Using these images, Kubernetes pods are spun up to execute the entry points or commands defined, tapping on to the Kubernetes cluster's available resources. Any jobs that are submitted to the Polyaxon server can be tracked and monitored through Polyaxon's dashboard. See this section on how to access the dashboard for Polyaxon and create a project.","title":"Job Orchestration"},{"location":"guide-for-user/07-job-orchestration/#pipeline-configuration","text":"In this template, Hydra is the configuration framework of choice for the data preparation and model training pipelines (or any pipelines that doesn't belong to the model serving aspects). The configurations for logging, pipelines and hyperparameter tuning can be found under conf/base . These YAML files are then referred to by Hydra or general utility functions ( src/{{cookiecutter.src_package_name}}/general_utils.py ) for loading of parameters and configurations. The defined default values can be overridden through the CLI. Attention It is recommended that you have a basic understanding of Hydra 's concepts before you move on. Reference(s): Hydra Docs - Basic Override Syntax","title":"Pipeline Configuration"},{"location":"guide-for-user/07-job-orchestration/#data-preparation-preprocessing","text":"To process the sample raw data, we will be spinning up a job on Polyaxon. This job will be using a Docker image that will be built from a Dockerfile ( docker/{{cookiecutter.repo_name}}-data-prep.Dockerfile ) provided in this template: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /data-prep:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -data-prep.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /data-prep:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / data-prep : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -data-prep . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / data-prep : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /data-prep:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -data-prep.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /data-prep:0.1.0 Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ data-prep : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -data-prep . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ data-prep : 0 . 1 . 0 {% endif %} Assuming you're still connected to the Polyaxon server through port-forwarding, submit a job to the server like such: Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/process-data.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /data-prep:0.1.0\" \\ -P RAW_DATA_DIRS = '[\"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1\"]' \\ -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / process -data . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/data-prep:0.1.0\" ` -P RAW_DATA_DIRS = \"['/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1']\" ` -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/process-data.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/data-prep:0.1.0\" \\ -P RAW_DATA_DIRS = '[\"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1\"]' \\ -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / process -data . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/data-prep:0.1.0\" ` -P RAW_DATA_DIRS = \"['/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/acl-movie-review-data-aisg/aclImdb-aisg-set1']\" ` -P PROCESSED_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} Info If you were to inspect aisg-context/polyaxon/polyaxonfiles/process-data.yml , the second command with yq overwrites the list of directories specified in the config file conf/base/pipelines.yml for the key .data_prep.raw_dirs_paths . You may specify a list of directory paths with which you can process and combine the results into one single directory. The yq utility is used to overwrite the values in the YAML config as Hydra currently doesn't support modification of list in YAML files . After some time, the data processing job should conclude and we can proceed with training the predictive model. The processed data is exported to the directory /polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined . We will be passing this path to the model training workflows.","title":"Data Preparation &amp; Preprocessing"},{"location":"guide-for-user/07-job-orchestration/#model-training","text":"Now that we have processed the raw data, we can look into training the sentiment classification model. The script relevant for this section is src/train_model.py . In this script, you can see it using some utility functions from src/{{cookiecutter.src_package_name}}/general_utils.py as well, most notably the functions for utilising MLflow utilities for tracking experiments. Let's set up the tooling for experiment tracking before we start model experimentation.","title":"Model Training"},{"location":"guide-for-user/07-job-orchestration/#experiment-tracking","text":"In the module src/{{cookiecutter.src_package_name}}/general_utils.py , the functions mlflow_init and mlflow_log are used to initialise MLflow experiments as well as log information and artifacts relevant for a run to a remote MLflow Tracking server. An MLflow Tracking server is usually set up within a GKE cluster for projects that requires model experimentation. Artifacts logged through the MLflow API can be uploaded to GCS buckets, assuming the client is authorised for access to GCS. Note The username and password for the MLflow Tracking server can be retrieved from the MLOps team or your team lead. To log and upload artifacts to GCS buckets through MLflow, you need to do the following first: Ensure that the Polyaxon job for model experimentation is configured to use your GCP project's service account credentials. See \"Secrets & Credentials on Kubernetes\" on how to do this. Create a bucket for storing such artifacts on GCS. Create an MLflow experiment on the tracking server. Let's create a GCS bucket for storing all your model experiment artifacts (assuming the bucket has yet to be created): Important For the purpose of this guide, we will create a bucket with the following name: gs://{{cookiecutter.repo_name}}-artifacts . The Docker images that we will be using to spin up inference servers will download exported model artifacts from this bucket, under a subdirectory mlflow-tracking-server . Hence, the longer path of the directory where all the model artifacts will be stored at will be gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server . Local Machine / Polyaxon VSCode Terminal $ gsutil mb -p {{ cookiecutter.gcp_project_id }} -c STANDARD -l ASIA-SOUTHEAST1 -b on gs:// {{ cookiecutter.repo_name }} -artifacts Creating gs:// {{ cookiecutter.repo_name }} -artifacts/... Now, let's access the MLflow Tracking server's dashboard to create an experiment for us to log runs to. Open a separate terminal and run the following: Local Machine $ kubectl port-forward service/mlflow-nginx-server-svc 5005 :5005 --namespace = polyaxon-v1 Head over to your web browser and access the following URL: http://localhost:5005 . You would be prompted for a username and password and upon a successful entry you should be presented with an interface similar to the one below: We are to collate runs under experiments. See here for the distinction between runs and experiments. For each experiment, we can specify paths and URLs for which we intend to upload artifacts to. To create an experiment, locate a + button on the top left hand corner of the interface. A pop-up box follows prompting a name for the experiment and a location for artifacts to be stored at. {% if cookiecutter.gcr_personal_subdir == 'No' %} For the current use case, let's make use of the following path for location of artifacts: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} For the current use case, let's make use of the path we have specified for MLflow artifacts, with your name appended: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}} . For name of experiment, you can specify a name like so: ml-experiment-{{cookiecutter.author_name}} . Warning Here are some things to take note when creating an experiment: Specifying object storage paths for a new experiment through MLflow's CLI does not work well currently so we would have to make do with creation of experiments through the UI. It is highly recommended that experiment names are without whitespaces . Words can be concatenated with hyphens. Reference(s): MLflow Docs - Tracking MLflow Docs - Tracking (Artifact Stores)","title":"Experiment Tracking"},{"location":"guide-for-user/07-job-orchestration/#container-for-experiment-job","text":"Before we submit a job to Polyaxon to train our model, we need to build the Docker image to be used for it: Linux/macOS $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -model-training-gpu.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0 Windows PowerShell $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / model-train : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -model-training-gpu . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID / model-train : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /model-train:0.1.0 \\ -f docker/ {{ cookiecutter.repo_name }} -model-training-gpu.Dockerfile \\ --platform linux/amd64 . $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /model-train:0.1.0 Windows PowerShell $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ model-train : 0 . 1 . 0 ` -f docker /{{ cookiecutter . repo_name }} -model-training-gpu . Dockerfile ` - -platform linux / amd64 . $ docker push asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ model-train : 0 . 1 . 0 {% endif %} Now that we have the Docker image pushed to the registry, we can run a job using it: Attention If the GKE cluster is without GPU nodes, the Polyaxonfiles would have to be edited to remove any request for GPUs (otherwise the job cannot be scheduled). Delete the following lines: ... tolerations : - key : \"nvidia.com/gpu\" operator : \"Equal\" value : \"present\" effect : \"NoSchedule\" ... nvidia.com/gpu : 1 ... {% if cookiecutter.gcr_personal_subdir == 'No' %} Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} !!! caution Do take note of the backtick (`) before the colon in the value for the parameter MLFLOW_TRACKING_URI . See here for some explanation.","title":"Container for Experiment Job"},{"location":"guide-for-user/07-job-orchestration/#hyperparameter-tuning","text":"For many ML problems, we would be bothered with finding the optimal parameters to train our models with. While we are able to override the parameters for our model training workflows, imagine having to sweep through a distribution of values. For example, if you were to seek for the optimal learning rate within a log space, we would have to execute polyaxon run a myriad of times manually, just to provide the training script with a different learning rate value each time. It is reasonable that one seeks for ways to automate this workflow. Optuna is an optimisation framework designed for ML use-cases. Its features includes: ease of modularity, optimisation algorithms for searching the best set of parameters, and paralellisation capabilities for faster sweeps. In addition, Hydra has a plugin for utilising Optuna which further translates to ease of configuration. To use Hydra's plugin for Optuna, we have to provide further overrides within the YAML config, and this is observed in conf/base/train-model-hptuning.yml : defaults : - override hydra/sweeper : \"optuna\" - override hydra/sweeper/sampler : \"tpe\" hydra : sweeper : sampler : seed : 123 direction : [ \"minimize\" , \"maximize\" ] study_name : \"sentiment-classification\" storage : null n_trials : 3 n_jobs : 1 search_space : train.val_split : type : \"float\" low : 0.2 high : 0.35 step : 0.025 train.optimiser : type : \"categorical\" choices : [ \"adam\" , \"rmsprop\" ] Attention The fields defined are terminologies used by Optuna. Therefore, it is recommended that you understand the basics of the tool. This overview video covers well on the concepts brought upon by Optuna. The script with which hyperparameter tuning is conducted, src/train_model_hptuning.py , has 2 essential lines that are different from src/train_model.py : ... @hydra . main ( config_path = \"../conf/base\" , config_name = \"train-model-hptuning.yml\" ) ... return test_loss , test_acc The first change specifies the different config file needed for utilising the Optuna plugin. The second one is needed for Optuna to judge the performance of the objectives (i.e. metrics) within each iteration, or as they put it, \"trials\". Another difference with this workflow is that for each trial with a different set of parameters, a new MLflow run has to be initialised. However, we need to somehow link all these different runs together so that we can compare all the runs within a single Optuna study (set of trials). How we do this is that we provide the script with a tag ( hptuning_tag ) which would essentially be the date epoch value of the moment you submitted the job to Polyaxon. This tag is defined using the environment value MLFLOW_HPTUNING_TAG . Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu-hptuning.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P MLFLOW_HPTUNING_TAG = \" $( date +%s ) \" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu-hptuning . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P MLFLOW_HPTUNING_TAG =$( Get-Date -UFormat % s -Millisecond 0 ) ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export MLFLOW_TRACKING_USERNAME = <MLFLOW_TRACKING_USERNAME> $ export MLFLOW_TRACKING_PASSWORD = <MLFLOW_TRACKING_PASSWORD> $ export CLUSTER_IP_OF_MLFLOW_SERVICE = $( kubectl get service/mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' --namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context/polyaxon/polyaxonfiles/train-model-gpu-hptuning.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/ $GCP_PROJECT_ID /{{cookiecutter.author_name}}/model-train:0.1.0\" \\ -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD \\ -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true \\ -P MLFLOW_TRACKING_URI = \"http:// $CLUSTER_IP_OF_MLFLOW_SERVICE :5005\" -P MLFLOW_EXP_NAME = <MLFLOW_EXPERIMENT_NAME> \\ -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" \\ -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" \\ -P MLFLOW_HPTUNING_TAG = \" $( date +%s ) \" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ $MLFLOW_TRACKING_USERNAME = '<MLFLOW_TRACKING_USERNAME>' $ $MLFLOW_TRACKING_PASSWORD = '<MLFLOW_TRACKING_PASSWORD>' $ $CLUSTER_IP_OF_MLFLOW_SERVICE =$( kubectl get service / mlflow-nginx-server-svc -o jsonpath = '{.spec.clusterIP}' - -namespace = polyaxon-v1 ) $ polyaxon run -f aisg-context / polyaxon / polyaxonfiles / train-model-gpu-hptuning . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/$GCP_PROJECT_ID/{{cookiecutter.author_name}}/model-train:0.1.0\" ` -P MLFLOW_TRACKING_USERNAME = $MLFLOW_TRACKING_USERNAME -P MLFLOW_TRACKING_PASSWORD = $MLFLOW_TRACKING_PASSWORD ` -P SETUP_MLFLOW = true -P MLFLOW_AUTOLOG = true ` -P MLFLOW_TRACKING_URI = \"http://$CLUSTER_IP_OF_MLFLOW_SERVICE`:5005\" -P MLFLOW_EXP_NAME =< MLFLOW_EXPERIMENT_NAME > ` -P WORKING_DIR = \"/home/aisg/{{cookiecutter.repo_name}}\" ` -P INPUT_DATA_DIR = \"/polyaxon-v1-data/workspaces/<YOUR_NAME>/data/processed/aclImdb-aisg-combined\" ` -P MLFLOW_HPTUNING_TAG =$( Get-Date -UFormat % s -Millisecond 0 ) ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} !!! caution Do take note of the backtick (`) before the colon in the value for the parameter MLFLOW_TRACKING_URI . See here for some explanation. Say the tag is 1641159546 , you can then filter this within MLflow's dashboard for runs with the associated tag like such in the search bar: tags.hptuning_tag=\"1641159546\" . Reference(s): Hydra Docs - Optuna Sweeper Plugin MLflow Docs - Search Syntax","title":"Hyperparameter Tuning"},{"location":"guide-for-user/08-deployment/","text":"Deployment \u00b6 Assuming we have a predictive model that we are satisfied with, we can serve it within a REST API service with which requests can be made to and predictions are returned. Python has plenty of web frameworks that we can leverage on to build our REST API. Popular examples include Flask , Django and Starlette . For this guide however, we will resort to the well-known FastAPI (which is based on Starlette itself). Reference(s): IBM Technology - What is a REST API? (Video) Model Artifacts \u00b6 Seen in \"Model Training\" , we have the trained models uploaded to GCS through the MLflow Tracking server (done through autolog). With that, we have the following pointers to take note of: By default, each MLflow experiment run is given a unique ID. When artifacts are uploaded to GCS through MLflow, the artifacts are located within directories named after the unique IDs of the runs. This guide by default uploads your artifacts to the following directory on GCS: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server . Artifacts for specific runs will be uploaded to a directory with a convention similar to the following: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/<MLFLOW_EXPERIMENT_UUID> . This guide by default uploads your artifacts to the following directory on GCS: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}} . Artifacts for specific runs will be uploaded to a directory with a convention similar to the following: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/<MLFLOW_EXPERIMENT_UUID> . With this path/URI, we can use gsutil to download the predictive model from GCS into a mounted volume when we run the Docker image for the REST APIs. Now that we have established on how we are to obtain the models for the API server, let's look into the servers themselves. Model Serving (FastAPI) \u00b6 FastAPI is a web framework that has garnered much popularity in recent years due to ease of adoption with its comprehensive tutorials, type and schema validation, being async capable and having automated docs, among other things. These factors have made it a popular framework within AI Singapore across many projects. If you were to inspect the src folder, you would notice that there exist more than one package: {{cookiecutter.src_package_name}} {{cookiecutter.src_package_name}}_fastapi The former contains the modules for executing pipelines like data preparation and model training while the latter is dedicated to modules meant for the REST API. Regardless, the packages can be imported by each other. Note It is recommended that you grasp some basics of the FastAPI framework, up till the beginner tutorials for better understanding of this section. Let's try running the boilerplate API server on a local machine. Before doing that, identify from the MLflow dashboard the unique ID of the experiment run that resulted in the predictive model that you would like to serve. With reference to the example screenshot above, the UUID for the experiment run is 7251ac3655934299aad4cfebf5ffddbe . Once the ID of the MLflow run has been obtained, let's download the model that we intend to serve. Assuming you're in the root of this template's repository, execute the following commands: Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% endif %} Executing the commands above will download the artifacts related to the experiment run <MLFLOW_EXPERIMENT_UUID> to this repository's subdirectory models . However, the specific subdirectory that is relevant for TensorFlow to load will be ./models/<MLFLOW_EXPERIMENT_UUID>/artifacts/model/data/model . Let's export this path to an environment variable: Note See here for more information on the SavedModel format that TensorFlow uses for exporting trained models through the function call model.save . Linux/macOS $ export PRED_MODEL_PATH = \" $PWD /models/ $PRED_MODEL_UUID /artifacts/model/data/model\" Windows PowerShell $ $Env:PRED_MODEL_PATH = \" $( Get-Location ) \\models\\$Env:PRED_MODEL_UUID\\artifacts\\model\\data\\model\" Local Server \u00b6 Run the FastAPI server using Gunicorn (for Linux/macOS) or uvicorn (for Windows): Attention Gunicorn is only executable on UNIX-based or UNIX-like systems, this method would not be possible/applicable for Windows machine. Linux/macOS $ conda activate {{ cookiecutter.repo_name }} $ cd src $ gunicorn {{ cookiecutter.src_package_name }} _fastapi.main:APP -b 0 .0.0.0:8080 -w 4 -k uvicorn.workers.UvicornWorker See here as to why Gunicorn is to be used instead of just Uvicorn . TLDR: Gunicorn is needed to spin up multiple processes/workers to handle more requests i.e. better for the sake of production needs. Windows PowerShell $ conda activate {{ cookiecutter . repo_name }} $ cd src $ uvicorn {{ cookiecutter . src_package_name }} _fastapi . main : APP In another terminal, use the curl command to submit a request to the API: Linux/macOS $ curl -H 'Content-Type: application/json' -H 'accept: application/json' \\ -X POST -d '{\"reviews\": [{\"id\": 9176, \"text\": \"This movie is quite boring.\"}, {\"id\": 71, \"text\": \"This movie is awesome.\"}]}' \\ localhost:8080/api/v1/model/predict Windows PowerShell $ curl . exe '-H' , 'Content-Type: application/json' , '-H' , 'accept: application/json' , ` '-X' , 'POST' , '-d' , ` '{\\\"reviews\\\": [{\\\"id\\\": 9176, \\\"text\\\": \\\"This movie is quite boring.\\\"}, {\\\"id\\\": 71, \\\"text\\\": \\\"This movie is awesome.\\\"}]}' , ` 'localhost:8000/api/v1/model/predict' With the returned JSON object, we have successfully submitted a request to the FastAPI server and it returned predictions as part of the response. Pydantic Settings \u00b6 Now you might be wondering, how does the FastAPI server knows the path to the model for it to load? FastAPI utilises Pydantic , a library for data and schema validation, as well as settings management . There's a class called Settings under the module src/{{cookiecutter.src_package_name}}_fastapi/config.py . This class contains several fields: some are defined and some others not. The fields PRED_MODEL_UUID and PRED_MODEL_PATH inherit their values from the environment variables. src/{{cookiecutter.src_package_name}}_fastapi/config.py : ... class Settings ( pydantic . BaseSettings ): API_NAME : str = \"{{cookiecutter.src_package_name}}_fastapi\" API_V1_STR : str = \"/api/v1\" LOGGER_CONFIG_PATH : str = \"../conf/base/logging.yml\" PRED_MODEL_UUID : str PRED_MODEL_PATH : str ... FastAPI automatically generates interactive API documentation for easy viewing of all the routers/endpoints you have made available for the server. You can view the documentation through <API_SERVER_URL>:<PORT>/docs . In our case here, it is viewable through localhost:8080/docs . It will look like such: Docker Container \u00b6 We now look into packaging the server within a Docker image. This process of containerising the server isn't just for the sake of reproducibility but it makes it easier for the server to be deployed on any server/infrastructure that can run a Docker container. A boilerplate Dockerfile is provided to containerise the FastAPI server: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} # Ensure that you are in the root of the repository $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /fastapi-server:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -fastapi.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' # Ensure that you are in the root of the repository $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / fastapi-server : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -fastapi . Dockerfile ` - -platform linux / amd64 . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} # Ensure that you are in the root of the repository $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /fastapi-server:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -fastapi.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' # Ensure that you are in the root of the repository $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ fastapi-server : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -fastapi . Dockerfile ` - -platform linux / amd64 . {% endif %} The Docker build command above requires an argument to be passed and it is basically the same unique MLflow run ID that was used above. The ID would then be used to create environment variables that would persist beyond the build time. When the container is being run, these environment variables would be used by the entrypoint script ( scripts/fastapi/api-entrypoint.sh ) to download the relevant predictive model into the mounted volumes and be referred to by the FastAPI Pydantic models. Let's try running the Docker container now: Linux/macOS # First make the `models` folder accessible to user within Docker container $ sudo chgrp -R 2222 models $ docker run --rm -p 8080 :8080 \\ --name fastapi-server \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ asia.gcr.io/ $GCP_PROJECT_ID /fastapi-server:0.1.0 Windows PowerShell $ docker run - -rm -p 8080 : 8080 ` - -name fastapi-server ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` - -env GOOGLE_APPLICATION_CREDENTIALS = \"/var/secret/cloud.google.com/gcp-service-account.json\" ` asia . gcr . io / $GCP_PROJECT_ID / fastapi-server : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS # First make the `models` folder accessible to user within Docker container $ sudo chgrp -R 2222 models $ docker run --rm -p 8080 :8080 \\ --name fastapi-server \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /fastapi-server:0.1.0 Windows PowerShell $ docker run - -rm -p 8080 : 8080 ` - -name fastapi-server ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` - -env GOOGLE_APPLICATION_CREDENTIALS = \"/var/secret/cloud.google.com/gcp-service-account.json\" ` asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ fastapi-server : 0 . 1 . 0 {% endif %} Let's go through a couple of the flags used above: --rm : Automatically stops the container when it exits or when you stop it. -p : This binds port 8080 of the container to port 8080 of the host machine. -v : Bind mounts files or directories from the host machine to the container. In this case, we are mounting the SA file and the models folder to the container. The SA file is needed for gsutil to download the model from GCS and the models folder will persist the downloaded models. --env : This sets environment variables within the container. Use the same curl command for the server spun up by the container: Linux/macOS $ curl -H 'Content-Type: application/json' -H 'accept: application/json' \\ -X POST -d '{\"reviews\": [{\"id\": 9176, \"text\": \"This movie is quite boring.\"}, {\"id\": 71, \"text\": \"This movie is awesome.\"}]}' \\ localhost:8080/api/v1/model/predict Windows PowerShell $ curl . exe '-H' , 'Content-Type: application/json' , '-H' , 'accept: application/json' , ` '-X' , 'POST' , '-d' , ` '{\\\"reviews\\\": [{\\\"id\\\": 9176, \\\"text\\\": \\\"This movie is quite boring.\\\"}, {\\\"id\\\": 71, \\\"text\\\": \\\"This movie is awesome.\\\"}]}' , ` 'localhost:8080/api/v1/model/predict' To stop the container: $ docker container stop fastapi-server Push the Docker image to the GCR: {% if cookiecutter.gcr_personal_subdir == 'No' %} $ docker push asia.gcr.io/ $GCP_PROJECT_ID /fastapi-server:0.1.0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /fastapi-server:0.1.0 {% endif %} With this Docker image, you can spin up a VM (Compute Engine instance) that has Docker installed and run the container on it for deployment. You can also deploy the image within a Kubernetes cluster for ease of scaling. Deploy to GKE \u00b6 Warning As this mode of deployment would take up resources in a long-running manner, please tear it down once you've gone through this part of the guide. If you do not have the right permissions, please request assistance from your team lead or the administrators. To deploy the FastAPI server on GKE, you can make use of the sample Kubernetes manifest files provided with this template: Local Machine $ kubectl apply -f aisg-context/k8s/model-serving-api/fastapi-server-deployment.yml --namespace = polyaxon-v1 $ kubectl apply -f aisg-context/k8s/model-serving-api/fastapi-server-service.yml --namespace = polyaxon-v1 To access the server, you can port-forward the service to a local port like such: Local Machine $ kubectl port-forward service/fastapi-server-svc 8080 :8080 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8080 -> 8080 Forwarding from [ ::1 ] :8080 -> 8080 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl port-forward service/fastapi-server- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc 8080 :8080 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8080 -> 8080 Forwarding from [ ::1 ] :8080 -> 8080 {% endif %} You can view the documentation for the API at http://localhost:8080/docs . You can also make a request to the API like so: Linux/macOS $ curl -H 'Content-Type: application/json' -H 'accept: application/json' \\ -X POST -d '{\"reviews\": [{\"id\": 9176, \"text\": \"This movie is quite boring.\"}, {\"id\": 71, \"text\": \"This movie is awesome.\"}]}' \\ localhost:8080/api/v1/model/predict Windows PowerShell $ curl . exe '-H' , 'Content-Type: application/json' , '-H' , 'accept: application/json' , ` '-X' , 'POST' , '-d' , ` '{\\\"reviews\\\": [{\\\"id\\\": 9176, \\\"text\\\": \\\"This movie is quite boring.\\\"}, {\\\"id\\\": 71, \\\"text\\\": \\\"This movie is awesome.\\\"}]}' , ` 'localhost:8080/api/v1/model/predict' Attention Please tear down the deployment and service objects once they are not required. Local Machine $ kubectl delete fastapi-server-deployment --namespace = polyaxon-v1 $ kubectl delete fastapi-server-svc --namespace = polyaxon-v1 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl delete fastapi-server- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -deployment --namespace = polyaxon-v1 $ kubectl delete fastapi-server- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc --namespace = polyaxon-v1 {% endif %} If you do not have the right permissions, please request assistance from your team lead or the administrators. Reference(s): FastAPI Docs Pydantic Docs - Settings Management TensorFlow Docs - tf.keras.models.load_model curl tutorial docker run Reference","title":"Deployment"},{"location":"guide-for-user/08-deployment/#deployment","text":"Assuming we have a predictive model that we are satisfied with, we can serve it within a REST API service with which requests can be made to and predictions are returned. Python has plenty of web frameworks that we can leverage on to build our REST API. Popular examples include Flask , Django and Starlette . For this guide however, we will resort to the well-known FastAPI (which is based on Starlette itself). Reference(s): IBM Technology - What is a REST API? (Video)","title":"Deployment"},{"location":"guide-for-user/08-deployment/#model-artifacts","text":"Seen in \"Model Training\" , we have the trained models uploaded to GCS through the MLflow Tracking server (done through autolog). With that, we have the following pointers to take note of: By default, each MLflow experiment run is given a unique ID. When artifacts are uploaded to GCS through MLflow, the artifacts are located within directories named after the unique IDs of the runs. This guide by default uploads your artifacts to the following directory on GCS: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server . Artifacts for specific runs will be uploaded to a directory with a convention similar to the following: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/<MLFLOW_EXPERIMENT_UUID> . This guide by default uploads your artifacts to the following directory on GCS: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}} . Artifacts for specific runs will be uploaded to a directory with a convention similar to the following: gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/<MLFLOW_EXPERIMENT_UUID> . With this path/URI, we can use gsutil to download the predictive model from GCS into a mounted volume when we run the Docker image for the REST APIs. Now that we have established on how we are to obtain the models for the API server, let's look into the servers themselves.","title":"Model Artifacts"},{"location":"guide-for-user/08-deployment/#model-serving-fastapi","text":"FastAPI is a web framework that has garnered much popularity in recent years due to ease of adoption with its comprehensive tutorials, type and schema validation, being async capable and having automated docs, among other things. These factors have made it a popular framework within AI Singapore across many projects. If you were to inspect the src folder, you would notice that there exist more than one package: {{cookiecutter.src_package_name}} {{cookiecutter.src_package_name}}_fastapi The former contains the modules for executing pipelines like data preparation and model training while the latter is dedicated to modules meant for the REST API. Regardless, the packages can be imported by each other. Note It is recommended that you grasp some basics of the FastAPI framework, up till the beginner tutorials for better understanding of this section. Let's try running the boilerplate API server on a local machine. Before doing that, identify from the MLflow dashboard the unique ID of the experiment run that resulted in the predictive model that you would like to serve. With reference to the example screenshot above, the UUID for the experiment run is 7251ac3655934299aad4cfebf5ffddbe . Once the ID of the MLflow run has been obtained, let's download the model that we intend to serve. Assuming you're in the root of this template's repository, execute the following commands: Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% endif %} Executing the commands above will download the artifacts related to the experiment run <MLFLOW_EXPERIMENT_UUID> to this repository's subdirectory models . However, the specific subdirectory that is relevant for TensorFlow to load will be ./models/<MLFLOW_EXPERIMENT_UUID>/artifacts/model/data/model . Let's export this path to an environment variable: Note See here for more information on the SavedModel format that TensorFlow uses for exporting trained models through the function call model.save . Linux/macOS $ export PRED_MODEL_PATH = \" $PWD /models/ $PRED_MODEL_UUID /artifacts/model/data/model\" Windows PowerShell $ $Env:PRED_MODEL_PATH = \" $( Get-Location ) \\models\\$Env:PRED_MODEL_UUID\\artifacts\\model\\data\\model\"","title":"Model Serving (FastAPI)"},{"location":"guide-for-user/08-deployment/#local-server","text":"Run the FastAPI server using Gunicorn (for Linux/macOS) or uvicorn (for Windows): Attention Gunicorn is only executable on UNIX-based or UNIX-like systems, this method would not be possible/applicable for Windows machine. Linux/macOS $ conda activate {{ cookiecutter.repo_name }} $ cd src $ gunicorn {{ cookiecutter.src_package_name }} _fastapi.main:APP -b 0 .0.0.0:8080 -w 4 -k uvicorn.workers.UvicornWorker See here as to why Gunicorn is to be used instead of just Uvicorn . TLDR: Gunicorn is needed to spin up multiple processes/workers to handle more requests i.e. better for the sake of production needs. Windows PowerShell $ conda activate {{ cookiecutter . repo_name }} $ cd src $ uvicorn {{ cookiecutter . src_package_name }} _fastapi . main : APP In another terminal, use the curl command to submit a request to the API: Linux/macOS $ curl -H 'Content-Type: application/json' -H 'accept: application/json' \\ -X POST -d '{\"reviews\": [{\"id\": 9176, \"text\": \"This movie is quite boring.\"}, {\"id\": 71, \"text\": \"This movie is awesome.\"}]}' \\ localhost:8080/api/v1/model/predict Windows PowerShell $ curl . exe '-H' , 'Content-Type: application/json' , '-H' , 'accept: application/json' , ` '-X' , 'POST' , '-d' , ` '{\\\"reviews\\\": [{\\\"id\\\": 9176, \\\"text\\\": \\\"This movie is quite boring.\\\"}, {\\\"id\\\": 71, \\\"text\\\": \\\"This movie is awesome.\\\"}]}' , ` 'localhost:8000/api/v1/model/predict' With the returned JSON object, we have successfully submitted a request to the FastAPI server and it returned predictions as part of the response.","title":"Local Server"},{"location":"guide-for-user/08-deployment/#pydantic-settings","text":"Now you might be wondering, how does the FastAPI server knows the path to the model for it to load? FastAPI utilises Pydantic , a library for data and schema validation, as well as settings management . There's a class called Settings under the module src/{{cookiecutter.src_package_name}}_fastapi/config.py . This class contains several fields: some are defined and some others not. The fields PRED_MODEL_UUID and PRED_MODEL_PATH inherit their values from the environment variables. src/{{cookiecutter.src_package_name}}_fastapi/config.py : ... class Settings ( pydantic . BaseSettings ): API_NAME : str = \"{{cookiecutter.src_package_name}}_fastapi\" API_V1_STR : str = \"/api/v1\" LOGGER_CONFIG_PATH : str = \"../conf/base/logging.yml\" PRED_MODEL_UUID : str PRED_MODEL_PATH : str ... FastAPI automatically generates interactive API documentation for easy viewing of all the routers/endpoints you have made available for the server. You can view the documentation through <API_SERVER_URL>:<PORT>/docs . In our case here, it is viewable through localhost:8080/docs . It will look like such:","title":"Pydantic Settings"},{"location":"guide-for-user/08-deployment/#docker-container","text":"We now look into packaging the server within a Docker image. This process of containerising the server isn't just for the sake of reproducibility but it makes it easier for the server to be deployed on any server/infrastructure that can run a Docker container. A boilerplate Dockerfile is provided to containerise the FastAPI server: Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} # Ensure that you are in the root of the repository $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID /fastapi-server:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -fastapi.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' # Ensure that you are in the root of the repository $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID / fastapi-server : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -fastapi . Dockerfile ` - -platform linux / amd64 . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export GCP_PROJECT_ID ={{ cookiecutter.gcp_project_id }} # Ensure that you are in the root of the repository $ docker build \\ -t asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /fastapi-server:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -fastapi.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $GCP_PROJECT_ID = '{{cookiecutter.gcp_project_id}}' # Ensure that you are in the root of the repository $ docker build ` -t asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ fastapi-server : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -fastapi . Dockerfile ` - -platform linux / amd64 . {% endif %} The Docker build command above requires an argument to be passed and it is basically the same unique MLflow run ID that was used above. The ID would then be used to create environment variables that would persist beyond the build time. When the container is being run, these environment variables would be used by the entrypoint script ( scripts/fastapi/api-entrypoint.sh ) to download the relevant predictive model into the mounted volumes and be referred to by the FastAPI Pydantic models. Let's try running the Docker container now: Linux/macOS # First make the `models` folder accessible to user within Docker container $ sudo chgrp -R 2222 models $ docker run --rm -p 8080 :8080 \\ --name fastapi-server \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ asia.gcr.io/ $GCP_PROJECT_ID /fastapi-server:0.1.0 Windows PowerShell $ docker run - -rm -p 8080 : 8080 ` - -name fastapi-server ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` - -env GOOGLE_APPLICATION_CREDENTIALS = \"/var/secret/cloud.google.com/gcp-service-account.json\" ` asia . gcr . io / $GCP_PROJECT_ID / fastapi-server : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS # First make the `models` folder accessible to user within Docker container $ sudo chgrp -R 2222 models $ docker run --rm -p 8080 :8080 \\ --name fastapi-server \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /fastapi-server:0.1.0 Windows PowerShell $ docker run - -rm -p 8080 : 8080 ` - -name fastapi-server ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` - -env GOOGLE_APPLICATION_CREDENTIALS = \"/var/secret/cloud.google.com/gcp-service-account.json\" ` asia . gcr . io / $GCP_PROJECT_ID /{{ cookiecutter . author_name }}/ fastapi-server : 0 . 1 . 0 {% endif %} Let's go through a couple of the flags used above: --rm : Automatically stops the container when it exits or when you stop it. -p : This binds port 8080 of the container to port 8080 of the host machine. -v : Bind mounts files or directories from the host machine to the container. In this case, we are mounting the SA file and the models folder to the container. The SA file is needed for gsutil to download the model from GCS and the models folder will persist the downloaded models. --env : This sets environment variables within the container. Use the same curl command for the server spun up by the container: Linux/macOS $ curl -H 'Content-Type: application/json' -H 'accept: application/json' \\ -X POST -d '{\"reviews\": [{\"id\": 9176, \"text\": \"This movie is quite boring.\"}, {\"id\": 71, \"text\": \"This movie is awesome.\"}]}' \\ localhost:8080/api/v1/model/predict Windows PowerShell $ curl . exe '-H' , 'Content-Type: application/json' , '-H' , 'accept: application/json' , ` '-X' , 'POST' , '-d' , ` '{\\\"reviews\\\": [{\\\"id\\\": 9176, \\\"text\\\": \\\"This movie is quite boring.\\\"}, {\\\"id\\\": 71, \\\"text\\\": \\\"This movie is awesome.\\\"}]}' , ` 'localhost:8080/api/v1/model/predict' To stop the container: $ docker container stop fastapi-server Push the Docker image to the GCR: {% if cookiecutter.gcr_personal_subdir == 'No' %} $ docker push asia.gcr.io/ $GCP_PROJECT_ID /fastapi-server:0.1.0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} $ docker push asia.gcr.io/ $GCP_PROJECT_ID / {{ cookiecutter.author_name }} /fastapi-server:0.1.0 {% endif %} With this Docker image, you can spin up a VM (Compute Engine instance) that has Docker installed and run the container on it for deployment. You can also deploy the image within a Kubernetes cluster for ease of scaling.","title":"Docker Container"},{"location":"guide-for-user/08-deployment/#deploy-to-gke","text":"Warning As this mode of deployment would take up resources in a long-running manner, please tear it down once you've gone through this part of the guide. If you do not have the right permissions, please request assistance from your team lead or the administrators. To deploy the FastAPI server on GKE, you can make use of the sample Kubernetes manifest files provided with this template: Local Machine $ kubectl apply -f aisg-context/k8s/model-serving-api/fastapi-server-deployment.yml --namespace = polyaxon-v1 $ kubectl apply -f aisg-context/k8s/model-serving-api/fastapi-server-service.yml --namespace = polyaxon-v1 To access the server, you can port-forward the service to a local port like such: Local Machine $ kubectl port-forward service/fastapi-server-svc 8080 :8080 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8080 -> 8080 Forwarding from [ ::1 ] :8080 -> 8080 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl port-forward service/fastapi-server- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc 8080 :8080 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8080 -> 8080 Forwarding from [ ::1 ] :8080 -> 8080 {% endif %} You can view the documentation for the API at http://localhost:8080/docs . You can also make a request to the API like so: Linux/macOS $ curl -H 'Content-Type: application/json' -H 'accept: application/json' \\ -X POST -d '{\"reviews\": [{\"id\": 9176, \"text\": \"This movie is quite boring.\"}, {\"id\": 71, \"text\": \"This movie is awesome.\"}]}' \\ localhost:8080/api/v1/model/predict Windows PowerShell $ curl . exe '-H' , 'Content-Type: application/json' , '-H' , 'accept: application/json' , ` '-X' , 'POST' , '-d' , ` '{\\\"reviews\\\": [{\\\"id\\\": 9176, \\\"text\\\": \\\"This movie is quite boring.\\\"}, {\\\"id\\\": 71, \\\"text\\\": \\\"This movie is awesome.\\\"}]}' , ` 'localhost:8080/api/v1/model/predict' Attention Please tear down the deployment and service objects once they are not required. Local Machine $ kubectl delete fastapi-server-deployment --namespace = polyaxon-v1 $ kubectl delete fastapi-server-svc --namespace = polyaxon-v1 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl delete fastapi-server- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -deployment --namespace = polyaxon-v1 $ kubectl delete fastapi-server- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc --namespace = polyaxon-v1 {% endif %} If you do not have the right permissions, please request assistance from your team lead or the administrators. Reference(s): FastAPI Docs Pydantic Docs - Settings Management TensorFlow Docs - tf.keras.models.load_model curl tutorial docker run Reference","title":"Deploy to GKE"},{"location":"guide-for-user/09-batch-inferencing/","text":"Batch Inferencing \u00b6 Some problem statements do not warrant the deployment of an API server but instead methods for conducting batched inferencing where a batch of data is provided to a script and the script churns out a set of predictions, perhaps exported to a file. This template provides a Python script ( src/batch_inferencing.py ) as well as an accompanying Dockerfile ( docker/{{cookiecutter.repo_name}}-batch-inferencing.Dockerfile ) for a containerised execution. Let's first download some data for us to conduct batch inferencing on: Local Machine $ mkdir data/batched-input-data $ cd data/batched-input-data $ gsutil -m rsync -r gs://aisg-mlops-pub-data/aclImdb_v1/detached/unsup . To execute the script locally: Linux/macOS # Navigate back to root directory $ cd ../.. $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_PATH = \" $PWD /models/ $PRED_MODEL_UUID /artifacts/model/data/model\" $ conda activate {{ cookiecutter.repo_name }} $ python src/batch_inferencing.py \\ inference.model_path = $PRED_MODEL_PATH \\ inference.input_data_dir = \" $PWD /data/batched-input-data\" Windows PowerShell # Navigate back to root directory $ cd ..\\.. $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $Env:PRED_MODEL_PATH = \" $( Get-Location ) \\models\\$Env:PRED_MODEL_UUID\\artifacts\\model\\data\\model\" $ conda activate {{ cookiecutter . repo_name }} $ python src / batch_inferencing . py ` inference . model_path = \"$Env:PRED_MODEL_PATH\" ` inference . input_data_dir = \" $( Get-Location ) \\data\\batched-input-data\" The parameter inference.input_data_dir assumes a directory containing .txt files containing movie reviews. At the end of the execution, the script will log to the terminal the location of the .jsonl file ( batch-infer-res.jsonl ) containing predictions that look like such: ... {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/1131_2.txt\", \"logit_prob\": 0.006387829780578613, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/11020_3.txt\", \"logit_prob\": 0.0041103363037109375, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/11916_3.txt\", \"logit_prob\": 0.023626357316970825, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/3129_2.txt\", \"logit_prob\": 0.00018364191055297852, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/2444_4.txt\", \"logit_prob\": 3.255962656112388e-05, \"sentiment\": \"negative\"} ... The results are exported to a subdirectory within the outputs folder. See here for more information on outputs generated by Hydra. To use the Docker image, first build it: Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /batch-inference:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -batch-inferencing.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ batch-inference : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -batch-inferencing . Dockerfile ` - -platform linux / amd64 . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /batch-inference:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -batch-inferencing.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ batch-inference : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -batch-inferencing . Dockerfile ` - -platform linux / amd64 . {% endif %} Similar to how the predictive models are defined for the FastAPI servers' images , PRED_MODEL_UUID requires the unique ID associated with the MLflow run that generated the predictive model that you wish to make use of for the batch inferencing. After building the image, you can run the container like so: Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ --env INPUT_DATA_DIR = /home/aisg/ {{ cookiecutter.repo_name }} /data \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ -v $PWD /outputs:/home/aisg/ {{ cookiecutter.repo_name }} /outputs \\ -v $PWD /data/batched-input-data:/home/aisg/ {{ cookiecutter.repo_name }} /data \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /batch-inference:0.1.0 Windows PowerShell $ docker run - -rm ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` - -env INPUT_DATA_DIR =/ home / aisg /{{ cookiecutter . repo_name }}/ data ` -v \"<ABSOLUTE_PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` -v \" $( Get-Location ) \\outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs\" ` -v \" $( Get-Location ) \\data\\batched-input-data:/home/aisg/{{cookiecutter.repo_name}}/data\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ batch-inference : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ --env INPUT_DATA_DIR = /home/aisg/ {{ cookiecutter.repo_name }} /data \\ -v <ABSOLUTE_PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ -v $PWD /outputs:/home/aisg/ {{ cookiecutter.repo_name }} /outputs \\ -v $PWD /data/batched-input-data:/home/aisg/ {{ cookiecutter.repo_name }} /data \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /batch-inference:0.1.0 Windows PowerShell $ docker run - -rm ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` - -env INPUT_DATA_DIR =/ home / aisg /{{ cookiecutter . repo_name }}/ data ` -v \"<ABSOLUTE_PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` -v \" $( Get-Location ) \\outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs\" ` -v \" $( Get-Location ) \\data\\batched-input-data:/home/aisg/{{cookiecutter.repo_name}}/data\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ batch-inference : 0 . 1 . 0 {% endif %} In the docker run command above we are passing two variables: GOOGLE_APPLICATION_CREDENTIALS and INPUT_DATA_DIR . The former allows the container's entrypoint to download the predictive model specified from GCS when the container starts. The latter will be fed to the script's parameter: inference.input_data_dir . 4 volumes are attached to the container for persistence as well as usage of host files and directories. -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json : This attaches the JSON file for the service account credentials to the Docker container. -v $PWD/models:/home/aisg/from-gcs : The models downloaded to the host machine can be used by the container after being mounted to /home/aisg/from-gcs . -v $PWD/outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs : This is for persisting the batch inferencing outputs to the outputs folder on the host machine. -v <PATH_TO_DIR_CONTAINING_TXT_FILES>:/home/aisg/{{cookiecutter.repo_name}}/data : To provide the container with access to the data that is on the host machine, you need to mount the directory containing the text files for inferencing. Reference(s): Docker Docs - Use volumes","title":"Batch Inferencing"},{"location":"guide-for-user/09-batch-inferencing/#batch-inferencing","text":"Some problem statements do not warrant the deployment of an API server but instead methods for conducting batched inferencing where a batch of data is provided to a script and the script churns out a set of predictions, perhaps exported to a file. This template provides a Python script ( src/batch_inferencing.py ) as well as an accompanying Dockerfile ( docker/{{cookiecutter.repo_name}}-batch-inferencing.Dockerfile ) for a containerised execution. Let's first download some data for us to conduct batch inferencing on: Local Machine $ mkdir data/batched-input-data $ cd data/batched-input-data $ gsutil -m rsync -r gs://aisg-mlops-pub-data/aclImdb_v1/detached/unsup . To execute the script locally: Linux/macOS # Navigate back to root directory $ cd ../.. $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_PATH = \" $PWD /models/ $PRED_MODEL_UUID /artifacts/model/data/model\" $ conda activate {{ cookiecutter.repo_name }} $ python src/batch_inferencing.py \\ inference.model_path = $PRED_MODEL_PATH \\ inference.input_data_dir = \" $PWD /data/batched-input-data\" Windows PowerShell # Navigate back to root directory $ cd ..\\.. $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $Env:PRED_MODEL_PATH = \" $( Get-Location ) \\models\\$Env:PRED_MODEL_UUID\\artifacts\\model\\data\\model\" $ conda activate {{ cookiecutter . repo_name }} $ python src / batch_inferencing . py ` inference . model_path = \"$Env:PRED_MODEL_PATH\" ` inference . input_data_dir = \" $( Get-Location ) \\data\\batched-input-data\" The parameter inference.input_data_dir assumes a directory containing .txt files containing movie reviews. At the end of the execution, the script will log to the terminal the location of the .jsonl file ( batch-infer-res.jsonl ) containing predictions that look like such: ... {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/1131_2.txt\", \"logit_prob\": 0.006387829780578613, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/11020_3.txt\", \"logit_prob\": 0.0041103363037109375, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/11916_3.txt\", \"logit_prob\": 0.023626357316970825, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/3129_2.txt\", \"logit_prob\": 0.00018364191055297852, \"sentiment\": \"negative\"} {\"time\": \"2022-01-06T06:40:27+0000\", \"filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/2444_4.txt\", \"logit_prob\": 3.255962656112388e-05, \"sentiment\": \"negative\"} ... The results are exported to a subdirectory within the outputs folder. See here for more information on outputs generated by Hydra. To use the Docker image, first build it: Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /batch-inference:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -batch-inferencing.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ batch-inference : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -batch-inferencing . Dockerfile ` - -platform linux / amd64 . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /batch-inference:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -batch-inferencing.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ batch-inference : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -batch-inferencing . Dockerfile ` - -platform linux / amd64 . {% endif %} Similar to how the predictive models are defined for the FastAPI servers' images , PRED_MODEL_UUID requires the unique ID associated with the MLflow run that generated the predictive model that you wish to make use of for the batch inferencing. After building the image, you can run the container like so: Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ --env INPUT_DATA_DIR = /home/aisg/ {{ cookiecutter.repo_name }} /data \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ -v $PWD /outputs:/home/aisg/ {{ cookiecutter.repo_name }} /outputs \\ -v $PWD /data/batched-input-data:/home/aisg/ {{ cookiecutter.repo_name }} /data \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /batch-inference:0.1.0 Windows PowerShell $ docker run - -rm ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` - -env INPUT_DATA_DIR =/ home / aisg /{{ cookiecutter . repo_name }}/ data ` -v \"<ABSOLUTE_PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` -v \" $( Get-Location ) \\outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs\" ` -v \" $( Get-Location ) \\data\\batched-input-data:/home/aisg/{{cookiecutter.repo_name}}/data\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ batch-inference : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ --env INPUT_DATA_DIR = /home/aisg/ {{ cookiecutter.repo_name }} /data \\ -v <ABSOLUTE_PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ -v $PWD /outputs:/home/aisg/ {{ cookiecutter.repo_name }} /outputs \\ -v $PWD /data/batched-input-data:/home/aisg/ {{ cookiecutter.repo_name }} /data \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /batch-inference:0.1.0 Windows PowerShell $ docker run - -rm ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` - -env INPUT_DATA_DIR =/ home / aisg /{{ cookiecutter . repo_name }}/ data ` -v \"<ABSOLUTE_PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` -v \" $( Get-Location ) \\outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs\" ` -v \" $( Get-Location ) \\data\\batched-input-data:/home/aisg/{{cookiecutter.repo_name}}/data\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ batch-inference : 0 . 1 . 0 {% endif %} In the docker run command above we are passing two variables: GOOGLE_APPLICATION_CREDENTIALS and INPUT_DATA_DIR . The former allows the container's entrypoint to download the predictive model specified from GCS when the container starts. The latter will be fed to the script's parameter: inference.input_data_dir . 4 volumes are attached to the container for persistence as well as usage of host files and directories. -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json : This attaches the JSON file for the service account credentials to the Docker container. -v $PWD/models:/home/aisg/from-gcs : The models downloaded to the host machine can be used by the container after being mounted to /home/aisg/from-gcs . -v $PWD/outputs:/home/aisg/{{cookiecutter.repo_name}}/outputs : This is for persisting the batch inferencing outputs to the outputs folder on the host machine. -v <PATH_TO_DIR_CONTAINING_TXT_FILES>:/home/aisg/{{cookiecutter.repo_name}}/data : To provide the container with access to the data that is on the host machine, you need to mount the directory containing the text files for inferencing. Reference(s): Docker Docs - Use volumes","title":"Batch Inferencing"},{"location":"guide-for-user/10-cicd/","text":"Continuous Integration & Deployment \u00b6 This template presents users with a base configuration for a GitLab CI/CD pipeline. In this section, the guide aims to provide readers with some basic understanding of the pipeline defined in the configuration file .gitlab-ci.yml . That being said, readers would certainly benefit from reading up on introductory CI/CD concepts as introduced by GitLab's Docs. GitHub Flow \u00b6 The defined pipeline assumes a GitHub flow which only relies on feature branches and a master / main (default) branch. With reference to the diagram above, we have the following pointers: We make use of feature branches ( git checkout -b <NAME_OF_BRANCH> ) to introduce changes to the source. Merge requests are made when we intend to merge the commits made to a feature branch to master . While one works on a feature branch, it is recommended that changes pushed to the master are pulled to the feature branch itself on a consistent basis. This allows the feature branch to possess the latest changes pushed by other developers through their own feature branches. In the example above, commits from the master branch following a merge of the add-hidden-layer branch are pulled into the change-training-image branch while that branch still expects further changes. The command git pull can be used to pull and sync these changes. However, it's recommended that developers make use of git fetch and git log to observe incoming changes first rather than pulling in changes in an indiscriminate manner. While it's possible for commits to be made directly to the master branch, it's recommended that they are kept minimal, at least for GitHub flow (other workflows might not heed such practices) . As we move along, we should be able to relate parts of the flow described above with the stages defined by the default GitLab CI pipeline. Environment Variables \u00b6 Before we can make use of the GitLab CI pipeline, we would have to define the following variables for the pipeline beforehand: GCP_PROJECT_ID : The ID of the GCP project for which container images are to be pushed to or where apps are to be deployed to. GCP_SERVICE_ACCOUNT_KEY : A service account's JSON key that is to be used for communicating with GCP services. PRED_MODEL_UUID : Unique ID of the MLflow run associated with a default model to be used for building the images of the following services: batch inferencing, FastAPI server and Streamlit app. Technically, this can be UUID for any baseline model or an arbitrary string as it can be overridden when the containers are being run. To define CI/CD variables for a project (repository), follow the steps listed here . For GCP_PROJECT_ID and PRED_MODEL_UUID , they are to be of Variable type while GCP_SERVICE_ACCOUNT_KEY needs to be a File type. Reference(s): GitLab Docs - GitLab CI/CD variables Stages & Jobs \u00b6 In the default pipeline, we have 3 stages defined: test : For every push to certain branches, the source code residing in src will be tested. build : Assuming the automated tests are passed, the pipeline will build Docker images, making use of the latest source. deploy-docs : This stage is for the purpose of deploying a static site through GitLab Pages . More on this stage is covered in \"Documentation\" . These stages are defined and listed like so: .gitlab-ci.yml ... stages : - test - build - deploy-docs ... The jobs for each of the stages are executed using Docker images defined by users. For this, we have to specify in the pipeline the tag associated with the GitLab Runner that has the Docker executor . In our case, the tag for the relevant runner is dind . .gitlab-ci.yml default : tags : - dind ... Automated Testing & Linting \u00b6 Let's look at the job defined for the test stage first: .gitlab-ci.yml ... test:pylint-pytest : stage : test image : name : continuumio/miniconda:4.7.12 before_script : - conda env create -f {{cookiecutter.repo_name}}-conda-env.yml - source activate {{cookiecutter.repo_name}} script : - pylint src --fail-under=7.0 --ignore=tests --disable=W1202 - pytest src/tests rules : - if : $CI_MERGE_REQUEST_IID changes : - src/**/* - conf/**/* - if : $CI_PIPELINE_SOURCE == \"push\" - if : $CI_COMMIT_TAG when : never ... First of all, this test:pylint-pytest job will only execute on the condition that the defined rules are met. In this case, the job will only execute for the following cases: For any pushes to any branch. For pushes to branches which merge requests have been created, tests are executed only if there are changes made to any files within src or conf are detected. This is to prevent automated tests from running for pushes made to feature branches with merge requests when no changes have been made to files for which tests are relevant. Otherwise, tests will run in a redundant manner, slowing down the feedback loop. If the push action is associated with a tag ( git push <remote> <tag_name> ), the job will not run. The job defined above fails under any of the following conditions: The source code does not meet a linting score of at least 6.5. The source code fails whatever tests have been defined under src/tests . The job would have to succeed before moving on to the build stage. Otherwise, no Docker images will be built. This is so that source code that fail tests would never be packaged. Reference(s): GitLab Docs - Predefined variables reference Real Python - Effective Python Testing With Pytest VSCode Docs - Linting Python in Visual Studio Code Automated Builds \u00b6 The template has thus far introduced a couple of Docker images relevant for the team. The tags for all the Docker images are listed below: {% if cookiecutter.gcr_personal_subdir == 'No' %} - asia.gcr.io/{{cookiecutter.gcp_project_id}}/vscode-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/jupyter-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/data-prep - asia.gcr.io/{{cookiecutter.gcp_project_id}}/model-train - asia.gcr.io/{{cookiecutter.gcp_project_id}}/fastapi-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/batch-inference - asia.gcr.io/{{cookiecutter.gcp_project_id}}/streamlit {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/vscode-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/jupyter-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/data-prep - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/model-train - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/fastapi-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/batch-inference - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/streamlit {% endif %} The build stage aims at automating the building of these Docker images in a parallel manner. Let's look at a snippet for a single job that builds a Docker image: .gitlab-ci.yml ... build:fastapi-server-image : stage : build image : name : gcr.io/kaniko-project/executor:debug entrypoint : [ \"\" ] variables : GOOGLE_APPLICATION_CREDENTIALS : /kaniko/gcp-sa.json script : - mkdir -p /kaniko/.docker - cat $GCP_SERVICE_ACCOUNT_KEY > /kaniko/gcp-sa.json - >- /kaniko/executor --context \"${CI_PROJECT_DIR}\" --dockerfile \"${CI_PROJECT_DIR}/docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile\" --build-arg PRED_MODEL_UUID=${PRED_MODEL_UUID} --destination \"asia.gcr.io/${GCP_PROJECT_ID}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" rules : - if : $CI_MERGE_REQUEST_IID changes : - docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile - src/**/* - conf/**/* - scripts/**/* - if : $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} .gitlab-ci.yml ... build:fastapi-server-image : stage : build image : name : gcr.io/kaniko-project/executor:debug entrypoint : [ \"\" ] variables : GOOGLE_APPLICATION_CREDENTIALS : /kaniko/gcp-sa.json script : - mkdir -p /kaniko/.docker - cat $GCP_SERVICE_ACCOUNT_KEY > /kaniko/gcp-sa.json - >- /kaniko/executor --context \"${CI_PROJECT_DIR}\" --dockerfile \"${CI_PROJECT_DIR}/docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile\" --build-arg PRED_MODEL_UUID=${PRED_MODEL_UUID} --destination \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" rules : - if : $CI_MERGE_REQUEST_IID changes : - docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile - src/**/* - conf/**/* - scripts/**/* - if : $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% endif %} Note You would have noticed that the jobs for building images utilise the command /kaniko/executor as opposed to docker build which most users would be more familiar with. This is due to the usage of kaniko within a runner with a Docker executor. Using Docker within Docker ( Docker-in-Docker ) requires privileged mode that poses several security concerns. Hence, the image gcr.io/kaniko-project/executor:debug is being used for all build jobs related to building of Docker images. That being said, the flags used for kaniko corresponds well with the flags usually used for docker commands. Just like with the test job, the each of the jobs under build will execute under certain conditions: If a push is being done to a branch which has a merge request opened, a check would be done to see if any changes were made to folders like src , conf , scripts , or the relevant Dockerfile itself. If there are changes, the job will be executed. An opened merge request is detected through the predefined variable CI_MERGE_REQUEST_IID . If a push is being made to the default branch ( CI_DEFAULT_BRANCH ) of the repo, which in most cases within our organisation would be master , the job would execute as well. Recalling the test stage, any pushes to the repo would trigger the automated tests and linting. If a push to the master branch passes the tests, all Docker images will be built, regardless of whether changes have been made to files relevant to the Docker images to be built themselves. Images built through the pipeline will be tagged with the commit hashes associated with the commits that triggered it. This is seen through the usage of the predefined variable CI_COMMIT_SHORT_SHA . Reference(s): GitLab Docs - Use kaniko to build Docker images GitLab Docs - Use Docker to build Docker images Tagging \u00b6 As mentioned, pushes to the default branch would trigger builds for Docker images and they would be tagged with the commit hash. However, such commit hashes aren't the best way to tag \"finalised\" Docker images so the usage of tags would be more appropriate here. Hence, for the job defined below, it would only trigger if a tag is pushed to the default branch and only the default branch. The tag pushed (say through a command like git push <remote> <tag> ) to the default branch on the remote would have the runner retag the Docker image that exists on GCR with the tag that is being pushed. The relevant images to be retagged are originally tagged with the short commit hash obtained from the commit that was pushed to the default branch before this. .gitlab-ci.yml ... build:retag-images : stage : build image : name : google/cloud-sdk:debian_component_based variables : GOOGLE_APPLICATION_CREDENTIALS : /gcp-sa.json script : - cat $GCP_SERVICE_ACCOUNT_KEY > /gcp-sa.json - gcloud auth activate-service-account --key-file=/gcp-sa.json - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/vscode-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/vscode-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/jupyter-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/jupyter-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/data-prep:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/data-prep:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/model-train:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/model-train:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/fastapi-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/batch-inference:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/batch-inference:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/streamlit:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/streamlit:${CI_COMMIT_TAG}\" rules : - if : $CI_COMMIT_TAG && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} .gitlab-ci.yml ... build:retag-images : stage : build image : name : google/cloud-sdk:debian_component_based variables : GOOGLE_APPLICATION_CREDENTIALS : /gcp-sa.json script : - cat $GCP_SERVICE_ACCOUNT_KEY > /gcp-sa.json - gcloud auth activate-service-account --key-file=/gcp-sa.json - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/vscode-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/vscode-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/jupyter-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/jupyter-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/data-prep:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/data-prep:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/model-train:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/model-train:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/fastapi-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/batch-inference:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/batch-inference:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/streamlit:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/streamlit:${CI_COMMIT_TAG}\" rules : - if : $CI_COMMIT_TAG && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% endif %} Reference(S): GitHub Docs - GitHub Flow GitLab Docs - GitLab Flow Conclusion \u00b6 The stages and jobs defined in this default pipeline is rudimentary at best as there is much more that could be done with GitLab CI. Some examples off the top: automatically generate reports for datasets that arrive in regular intervals submit model training jobs following triggers invoked by the same pipeline automate the deployment of the FastAPI servers to the GKE clusters that is accessible by the service account provided to the repository There's much more that can be done but whatever has been shared thus far is hopefully enough for one to get started with CI/CD.","title":"CI/CD"},{"location":"guide-for-user/10-cicd/#continuous-integration-deployment","text":"This template presents users with a base configuration for a GitLab CI/CD pipeline. In this section, the guide aims to provide readers with some basic understanding of the pipeline defined in the configuration file .gitlab-ci.yml . That being said, readers would certainly benefit from reading up on introductory CI/CD concepts as introduced by GitLab's Docs.","title":"Continuous Integration &amp; Deployment"},{"location":"guide-for-user/10-cicd/#github-flow","text":"The defined pipeline assumes a GitHub flow which only relies on feature branches and a master / main (default) branch. With reference to the diagram above, we have the following pointers: We make use of feature branches ( git checkout -b <NAME_OF_BRANCH> ) to introduce changes to the source. Merge requests are made when we intend to merge the commits made to a feature branch to master . While one works on a feature branch, it is recommended that changes pushed to the master are pulled to the feature branch itself on a consistent basis. This allows the feature branch to possess the latest changes pushed by other developers through their own feature branches. In the example above, commits from the master branch following a merge of the add-hidden-layer branch are pulled into the change-training-image branch while that branch still expects further changes. The command git pull can be used to pull and sync these changes. However, it's recommended that developers make use of git fetch and git log to observe incoming changes first rather than pulling in changes in an indiscriminate manner. While it's possible for commits to be made directly to the master branch, it's recommended that they are kept minimal, at least for GitHub flow (other workflows might not heed such practices) . As we move along, we should be able to relate parts of the flow described above with the stages defined by the default GitLab CI pipeline.","title":"GitHub Flow"},{"location":"guide-for-user/10-cicd/#environment-variables","text":"Before we can make use of the GitLab CI pipeline, we would have to define the following variables for the pipeline beforehand: GCP_PROJECT_ID : The ID of the GCP project for which container images are to be pushed to or where apps are to be deployed to. GCP_SERVICE_ACCOUNT_KEY : A service account's JSON key that is to be used for communicating with GCP services. PRED_MODEL_UUID : Unique ID of the MLflow run associated with a default model to be used for building the images of the following services: batch inferencing, FastAPI server and Streamlit app. Technically, this can be UUID for any baseline model or an arbitrary string as it can be overridden when the containers are being run. To define CI/CD variables for a project (repository), follow the steps listed here . For GCP_PROJECT_ID and PRED_MODEL_UUID , they are to be of Variable type while GCP_SERVICE_ACCOUNT_KEY needs to be a File type. Reference(s): GitLab Docs - GitLab CI/CD variables","title":"Environment Variables"},{"location":"guide-for-user/10-cicd/#stages-jobs","text":"In the default pipeline, we have 3 stages defined: test : For every push to certain branches, the source code residing in src will be tested. build : Assuming the automated tests are passed, the pipeline will build Docker images, making use of the latest source. deploy-docs : This stage is for the purpose of deploying a static site through GitLab Pages . More on this stage is covered in \"Documentation\" . These stages are defined and listed like so: .gitlab-ci.yml ... stages : - test - build - deploy-docs ... The jobs for each of the stages are executed using Docker images defined by users. For this, we have to specify in the pipeline the tag associated with the GitLab Runner that has the Docker executor . In our case, the tag for the relevant runner is dind . .gitlab-ci.yml default : tags : - dind ...","title":"Stages &amp; Jobs"},{"location":"guide-for-user/10-cicd/#automated-testing-linting","text":"Let's look at the job defined for the test stage first: .gitlab-ci.yml ... test:pylint-pytest : stage : test image : name : continuumio/miniconda:4.7.12 before_script : - conda env create -f {{cookiecutter.repo_name}}-conda-env.yml - source activate {{cookiecutter.repo_name}} script : - pylint src --fail-under=7.0 --ignore=tests --disable=W1202 - pytest src/tests rules : - if : $CI_MERGE_REQUEST_IID changes : - src/**/* - conf/**/* - if : $CI_PIPELINE_SOURCE == \"push\" - if : $CI_COMMIT_TAG when : never ... First of all, this test:pylint-pytest job will only execute on the condition that the defined rules are met. In this case, the job will only execute for the following cases: For any pushes to any branch. For pushes to branches which merge requests have been created, tests are executed only if there are changes made to any files within src or conf are detected. This is to prevent automated tests from running for pushes made to feature branches with merge requests when no changes have been made to files for which tests are relevant. Otherwise, tests will run in a redundant manner, slowing down the feedback loop. If the push action is associated with a tag ( git push <remote> <tag_name> ), the job will not run. The job defined above fails under any of the following conditions: The source code does not meet a linting score of at least 6.5. The source code fails whatever tests have been defined under src/tests . The job would have to succeed before moving on to the build stage. Otherwise, no Docker images will be built. This is so that source code that fail tests would never be packaged. Reference(s): GitLab Docs - Predefined variables reference Real Python - Effective Python Testing With Pytest VSCode Docs - Linting Python in Visual Studio Code","title":"Automated Testing &amp; Linting"},{"location":"guide-for-user/10-cicd/#automated-builds","text":"The template has thus far introduced a couple of Docker images relevant for the team. The tags for all the Docker images are listed below: {% if cookiecutter.gcr_personal_subdir == 'No' %} - asia.gcr.io/{{cookiecutter.gcp_project_id}}/vscode-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/jupyter-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/data-prep - asia.gcr.io/{{cookiecutter.gcp_project_id}}/model-train - asia.gcr.io/{{cookiecutter.gcp_project_id}}/fastapi-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/batch-inference - asia.gcr.io/{{cookiecutter.gcp_project_id}}/streamlit {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/vscode-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/jupyter-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/data-prep - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/model-train - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/fastapi-server - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/batch-inference - asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/streamlit {% endif %} The build stage aims at automating the building of these Docker images in a parallel manner. Let's look at a snippet for a single job that builds a Docker image: .gitlab-ci.yml ... build:fastapi-server-image : stage : build image : name : gcr.io/kaniko-project/executor:debug entrypoint : [ \"\" ] variables : GOOGLE_APPLICATION_CREDENTIALS : /kaniko/gcp-sa.json script : - mkdir -p /kaniko/.docker - cat $GCP_SERVICE_ACCOUNT_KEY > /kaniko/gcp-sa.json - >- /kaniko/executor --context \"${CI_PROJECT_DIR}\" --dockerfile \"${CI_PROJECT_DIR}/docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile\" --build-arg PRED_MODEL_UUID=${PRED_MODEL_UUID} --destination \"asia.gcr.io/${GCP_PROJECT_ID}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" rules : - if : $CI_MERGE_REQUEST_IID changes : - docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile - src/**/* - conf/**/* - scripts/**/* - if : $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} .gitlab-ci.yml ... build:fastapi-server-image : stage : build image : name : gcr.io/kaniko-project/executor:debug entrypoint : [ \"\" ] variables : GOOGLE_APPLICATION_CREDENTIALS : /kaniko/gcp-sa.json script : - mkdir -p /kaniko/.docker - cat $GCP_SERVICE_ACCOUNT_KEY > /kaniko/gcp-sa.json - >- /kaniko/executor --context \"${CI_PROJECT_DIR}\" --dockerfile \"${CI_PROJECT_DIR}/docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile\" --build-arg PRED_MODEL_UUID=${PRED_MODEL_UUID} --destination \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" rules : - if : $CI_MERGE_REQUEST_IID changes : - docker/{{cookiecutter.repo_name}}-fastapi.Dockerfile - src/**/* - conf/**/* - scripts/**/* - if : $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% endif %} Note You would have noticed that the jobs for building images utilise the command /kaniko/executor as opposed to docker build which most users would be more familiar with. This is due to the usage of kaniko within a runner with a Docker executor. Using Docker within Docker ( Docker-in-Docker ) requires privileged mode that poses several security concerns. Hence, the image gcr.io/kaniko-project/executor:debug is being used for all build jobs related to building of Docker images. That being said, the flags used for kaniko corresponds well with the flags usually used for docker commands. Just like with the test job, the each of the jobs under build will execute under certain conditions: If a push is being done to a branch which has a merge request opened, a check would be done to see if any changes were made to folders like src , conf , scripts , or the relevant Dockerfile itself. If there are changes, the job will be executed. An opened merge request is detected through the predefined variable CI_MERGE_REQUEST_IID . If a push is being made to the default branch ( CI_DEFAULT_BRANCH ) of the repo, which in most cases within our organisation would be master , the job would execute as well. Recalling the test stage, any pushes to the repo would trigger the automated tests and linting. If a push to the master branch passes the tests, all Docker images will be built, regardless of whether changes have been made to files relevant to the Docker images to be built themselves. Images built through the pipeline will be tagged with the commit hashes associated with the commits that triggered it. This is seen through the usage of the predefined variable CI_COMMIT_SHORT_SHA . Reference(s): GitLab Docs - Use kaniko to build Docker images GitLab Docs - Use Docker to build Docker images","title":"Automated Builds"},{"location":"guide-for-user/10-cicd/#tagging","text":"As mentioned, pushes to the default branch would trigger builds for Docker images and they would be tagged with the commit hash. However, such commit hashes aren't the best way to tag \"finalised\" Docker images so the usage of tags would be more appropriate here. Hence, for the job defined below, it would only trigger if a tag is pushed to the default branch and only the default branch. The tag pushed (say through a command like git push <remote> <tag> ) to the default branch on the remote would have the runner retag the Docker image that exists on GCR with the tag that is being pushed. The relevant images to be retagged are originally tagged with the short commit hash obtained from the commit that was pushed to the default branch before this. .gitlab-ci.yml ... build:retag-images : stage : build image : name : google/cloud-sdk:debian_component_based variables : GOOGLE_APPLICATION_CREDENTIALS : /gcp-sa.json script : - cat $GCP_SERVICE_ACCOUNT_KEY > /gcp-sa.json - gcloud auth activate-service-account --key-file=/gcp-sa.json - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/vscode-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/vscode-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/jupyter-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/jupyter-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/data-prep:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/data-prep:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/model-train:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/model-train:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/fastapi-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/batch-inference:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/batch-inference:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/streamlit:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/streamlit:${CI_COMMIT_TAG}\" rules : - if : $CI_COMMIT_TAG && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} .gitlab-ci.yml ... build:retag-images : stage : build image : name : google/cloud-sdk:debian_component_based variables : GOOGLE_APPLICATION_CREDENTIALS : /gcp-sa.json script : - cat $GCP_SERVICE_ACCOUNT_KEY > /gcp-sa.json - gcloud auth activate-service-account --key-file=/gcp-sa.json - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/vscode-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/vscode-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/jupyter-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/jupyter-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/data-prep:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/data-prep:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/model-train:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/model-train:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/fastapi-server:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/fastapi-server:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/batch-inference:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/batch-inference:${CI_COMMIT_TAG}\" - gcloud container images add-tag --quiet \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/streamlit:${CI_COMMIT_SHORT_SHA}\" \"asia.gcr.io/${GCP_PROJECT_ID}/{{cookiecutter.author_name}}/streamlit:${CI_COMMIT_TAG}\" rules : - if : $CI_COMMIT_TAG && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH ... {% endif %} Reference(S): GitHub Docs - GitHub Flow GitLab Docs - GitLab Flow","title":"Tagging"},{"location":"guide-for-user/10-cicd/#conclusion","text":"The stages and jobs defined in this default pipeline is rudimentary at best as there is much more that could be done with GitLab CI. Some examples off the top: automatically generate reports for datasets that arrive in regular intervals submit model training jobs following triggers invoked by the same pipeline automate the deployment of the FastAPI servers to the GKE clusters that is accessible by the service account provided to the repository There's much more that can be done but whatever has been shared thus far is hopefully enough for one to get started with CI/CD.","title":"Conclusion"},{"location":"guide-for-user/11-documentation/","text":"Documentation \u00b6 The boilerplate packages generated by the template are populated with some NumPy formatted docstrings . What we can do with this is to observe how documentation can be automatically generated using Sphinx , with the aid of the Napoleon extension. Let's build the HTML asset for the documentation: # From the root folder $ conda activate {{ cookiecutter.repo_name }} $ sphinx-apidoc -f -o docs src $ sphinx-build -b html docs public Open the file public/index.html with your browser and you will be presented with a static site similar to the one shown below: Browse through the site and inspect the documentation that was automatically generated through Sphinx. GitLab Pages \u00b6 Documentation generated through Sphinx can be served on GitLab Pages , through GitLab CI/CD. With this template, a default CI job has been defined in .gitlab-ci.yml to serve the Sphinx documentation when pushes are done to the master branch: ... pages : stage : deploy-docs image : name : continuumio/miniconda:4.7.12 script : - conda env update -f {{cookiecutter.repo_name}}-conda-env.yml - conda init bash - source ~/.bashrc - conda activate {{cookiecutter.repo_name}} - sphinx-apidoc -f -o docs src - sphinx-build -b html docs public artifacts : paths : - public only : - master ... The documentation page is viewable through the following convention: <NAMESPACE>.gitlab.aisingapore.net/<PROJECT_NAME> or <NAMESPACE>.gitlab.aisingapore.net/<GROUP>/<PROJECT_NAME> . Reference(s): GitLab Docs - Pages domain names, URLs, and base URLs GitLab Docs - Namespaces","title":"Documentation"},{"location":"guide-for-user/11-documentation/#documentation","text":"The boilerplate packages generated by the template are populated with some NumPy formatted docstrings . What we can do with this is to observe how documentation can be automatically generated using Sphinx , with the aid of the Napoleon extension. Let's build the HTML asset for the documentation: # From the root folder $ conda activate {{ cookiecutter.repo_name }} $ sphinx-apidoc -f -o docs src $ sphinx-build -b html docs public Open the file public/index.html with your browser and you will be presented with a static site similar to the one shown below: Browse through the site and inspect the documentation that was automatically generated through Sphinx.","title":"Documentation"},{"location":"guide-for-user/11-documentation/#gitlab-pages","text":"Documentation generated through Sphinx can be served on GitLab Pages , through GitLab CI/CD. With this template, a default CI job has been defined in .gitlab-ci.yml to serve the Sphinx documentation when pushes are done to the master branch: ... pages : stage : deploy-docs image : name : continuumio/miniconda:4.7.12 script : - conda env update -f {{cookiecutter.repo_name}}-conda-env.yml - conda init bash - source ~/.bashrc - conda activate {{cookiecutter.repo_name}} - sphinx-apidoc -f -o docs src - sphinx-build -b html docs public artifacts : paths : - public only : - master ... The documentation page is viewable through the following convention: <NAMESPACE>.gitlab.aisingapore.net/<PROJECT_NAME> or <NAMESPACE>.gitlab.aisingapore.net/<GROUP>/<PROJECT_NAME> . Reference(s): GitLab Docs - Pages domain names, URLs, and base URLs GitLab Docs - Namespaces","title":"GitLab Pages"},{"location":"guide-for-user/12-streamlit/","text":"Streamlit \u00b6 There are 4 main ways we recommend to spin up Streamlit applications for quick dashboarding: Local Execution Docker Container Integration with Polyaxon Native Kubernetes Deployment (GKE) The Streamlit demo created in this guide will accept a string as an input, and the dashboard will provide an output as to whether the sentiment is \"positive\" or \"negative\", following the guide's problem statement . This guide will be similar to that of \"Deployment\" and \"Batch Inferencing\" , with the difference mainly being the use of Streamlit as an interface to get your inputs and show your outputs from. While it is possible for Streamlit to interact with the FastAPI deployment backend as a frontend engine/interface, for simplicities' sake, we will only dealing with the use case where Streamlit application directly loads the predictive model downloaded from GCS. For small scale infrastructure or prototyping, this would be sufficient in terms of simplicity and efficiency. This template provides: a Python script ( src/streamlit.py ) a Dockerfile for containerised executions ( docker/{{cookiecutter.repo_name}}-streamlit.Dockerfile ) a Polyaxon config file for spinning up a Streamlit service ( aisg-context/polyaxon/polyaxonfiles/streamlit.yml ) Local Execution \u00b6 To run the Streamlit app locally, one of course has to download a predictive model into the local machine: Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% endif %} PRED_MODEL_UUID is the unique ID associated with the MLFLow run that generated the predictive model to be used for dashboarding. Spin up the Streamlit application locally: Linux/macOS $ export PRED_MODEL_PATH = \" $PWD /models/ $PRED_MODEL_UUID /artifacts/model/data/model\" $ streamlit run src/streamlit.py -- \\ hydra.run.dir = . hydra.output_subdir = null hydra/job_logging = disabled \\ inference.model_path = $PRED_MODEL_PATH Windows PowerShell $ $Env:PRED_MODEL_PATH = \" $( Get-Location ) \\models\\$Env:PRED_MODEL_UUID\\artifacts\\model\\data\\model\" $ streamlit run src / streamlit . py -- ` hydra . run . dir =. hydra . output_subdir = null hydra / job_logging = disabled ` inference . model_path = $Env:PRED_MODEL_PATH The application would look like the screenshot below: Reference(s): Streamlit Docs - Run Streamlit apps Docker Container \u00b6 To use the Docker image, first build it: Linux/macOS $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /streamlit:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -streamlit.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ streamlit : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -streamlit . Dockerfile ` - -platform linux / amd64 . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /streamlit:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -streamlit.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ streamlit : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -streamlit . Dockerfile ` - -platform linux / amd64 . {% endif %} After building the image, you can run the container like so: Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm -p 8501 :8501 \\ --name streamlit-app \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /streamlit:0.1.0 Windows PowerShell $ docker run - -rm -p 8501 : 8501 ` - -name streamlit-app ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ streamlit : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm -p 8501 :8501 \\ --name streamlit-app \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /streamlit:0.1.0 Windows PowerShell $ docker run - -rm -p 8501 : 8501 ` - -name streamlit-app ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ streamlit : 0 . 1 . 0 {% endif %} - GOOGLE_APPLICATION_CREDENTIALS allows the container's entrypoint to download the predictive model specified during build time from GCS. - -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json attaches the JSON file for the service account credentials to the Docker container. - -v $PWD/models:/home/aisg/from-gcs allows the models downloaded to the host machine to be used by the container after being mounted to /home/aisg/from-gcs . To stop the container: $ docker container stop streamlit-app Integration with Polyaxon \u00b6 Attention As this mode of deployment would take up resources in a long-running manner, please tear the service down through the dashboard once you've gone through this part of the guide. From the Docker build section, push the Docker image to GCR: {% if cookiecutter.gcr_personal_subdir == 'No' %} $ docker push asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /streamlit:0.1.0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} $ docker push asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /streamlit:0.1.0 {% endif %} Then, push the configurations to the Polyaxon server to start up the Streamlit dashboard: Linux/macOS $ polyaxon run \\ -f aisg-context/polyaxon/polyaxonfiles/streamlit-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/streamlit:0.1.0\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run ` -f aisg-context / polyaxon / polyaxonfiles / streamlit-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/streamlit:0.1.0\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run \\ -f aisg-context/polyaxon/polyaxonfiles/streamlit-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/streamlit:0.1.0\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run ` -f aisg-context / polyaxon / polyaxonfiles / streamlit-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/streamlit:0.1.0\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} Just like with the VSCode or JupyterLab services, you can access the Streamlit service you've just spun up through the Polyaxon dashboard: Reference(s): Polyaxon - Integrations Native Kubernetes Deployment (GKE) \u00b6 Attention As this mode of deployment would take up resources in a long-running manner, please tear it down once you've gone through this part of the guide. If you do not have the right permissions, please request assistance from your team lead or the administrators. Similar to deploying the FastAPI server , to deploy the Streamlit dashboard on GKE, you can make use of the sample Kubernetes manifest files provided with this template: $ kubectl apply -f aisg-context/k8s/dashboard/streamlit-deployment.yml --namespace = polyaxon-v1 $ kubectl apply -f aisg-context/k8s/dashboard/streamlit-service.yml --namespace = polyaxon-v1 To access the server, you can port-forward the service to a local port like such: Local Machine $ kubectl port-forward service/streamlit-svc 8501 :8501 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8501 -> 8501 Forwarding from [ ::1 ] :8501 -> 8501 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl port-forward service/streamlit- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc 8501 :8501 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8501 -> 8501 Forwarding from [ ::1 ] :8501 -> 8501 {% endif %} Attention Please tear down the deployment and service objects once they are not required. Local Machine $ kubectl delete streamlit-deployment --namespace = polyaxon-v1 $ kubectl delete streamlit-svc --namespace = polyaxon-v1 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl delete streamlit- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -deployment --namespace = polyaxon-v1 $ kubectl delete streamlit- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc --namespace = polyaxon-v1 {% endif %} If you do not have the right permissions, please request assistance from your team lead or the administrators.","title":"Streamlit"},{"location":"guide-for-user/12-streamlit/#streamlit","text":"There are 4 main ways we recommend to spin up Streamlit applications for quick dashboarding: Local Execution Docker Container Integration with Polyaxon Native Kubernetes Deployment (GKE) The Streamlit demo created in this guide will accept a string as an input, and the dashboard will provide an output as to whether the sentiment is \"positive\" or \"negative\", following the guide's problem statement . This guide will be similar to that of \"Deployment\" and \"Batch Inferencing\" , with the difference mainly being the use of Streamlit as an interface to get your inputs and show your outputs from. While it is possible for Streamlit to interact with the FastAPI deployment backend as a frontend engine/interface, for simplicities' sake, we will only dealing with the use case where Streamlit application directly loads the predictive model downloaded from GCS. For small scale infrastructure or prototyping, this would be sufficient in terms of simplicity and efficiency. This template provides: a Python script ( src/streamlit.py ) a Dockerfile for containerised executions ( docker/{{cookiecutter.repo_name}}-streamlit.Dockerfile ) a Polyaxon config file for spinning up a Streamlit service ( aisg-context/polyaxon/polyaxonfiles/streamlit.yml )","title":"Streamlit"},{"location":"guide-for-user/12-streamlit/#local-execution","text":"To run the Streamlit app locally, one of course has to download a predictive model into the local machine: Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ export PRED_MODEL_UUID = \"<MLFLOW_EXPERIMENT_UUID>\" $ export PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/ $PRED_MODEL_UUID \" $ gsutil cp -r $PRED_MODEL_GCS_URI ./models Windows PowerShell $ $Env:PRED_MODEL_UUID = '<MLFLOW_EXPERIMENT_UUID>' $ $PRED_MODEL_GCS_URI = \"gs://{{cookiecutter.repo_name}}-artifacts/mlflow-tracking-server/{{cookiecutter.author_name}}/$Env:PRED_MODEL_UUID\" $ gsutil cp -r $PRED_MODEL_GCS_URI .\\ models {% endif %} PRED_MODEL_UUID is the unique ID associated with the MLFLow run that generated the predictive model to be used for dashboarding. Spin up the Streamlit application locally: Linux/macOS $ export PRED_MODEL_PATH = \" $PWD /models/ $PRED_MODEL_UUID /artifacts/model/data/model\" $ streamlit run src/streamlit.py -- \\ hydra.run.dir = . hydra.output_subdir = null hydra/job_logging = disabled \\ inference.model_path = $PRED_MODEL_PATH Windows PowerShell $ $Env:PRED_MODEL_PATH = \" $( Get-Location ) \\models\\$Env:PRED_MODEL_UUID\\artifacts\\model\\data\\model\" $ streamlit run src / streamlit . py -- ` hydra . run . dir =. hydra . output_subdir = null hydra / job_logging = disabled ` inference . model_path = $Env:PRED_MODEL_PATH The application would look like the screenshot below: Reference(s): Streamlit Docs - Run Streamlit apps","title":"Local Execution"},{"location":"guide-for-user/12-streamlit/#docker-container","text":"To use the Docker image, first build it: Linux/macOS $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /streamlit:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -streamlit.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ streamlit : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -streamlit . Dockerfile ` - -platform linux / amd64 . {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ docker build \\ -t asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /streamlit:0.1.0 \\ --build-arg PRED_MODEL_UUID = \" $PRED_MODEL_UUID \" \\ -f docker/ {{ cookiecutter.repo_name }} -streamlit.Dockerfile \\ --platform linux/amd64 . Windows PowerShell $ docker build ` -t asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ streamlit : 0 . 1 . 0 ` - -build-arg PRED_MODEL_UUID = \"$Env:PRED_MODEL_UUID\" ` -f docker /{{ cookiecutter . repo_name }} -streamlit . Dockerfile ` - -platform linux / amd64 . {% endif %} After building the image, you can run the container like so: Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm -p 8501 :8501 \\ --name streamlit-app \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /streamlit:0.1.0 Windows PowerShell $ docker run - -rm -p 8501 : 8501 ` - -name streamlit-app ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/ streamlit : 0 . 1 . 0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ sudo chgrp -R 2222 outputs $ docker run --rm -p 8501 :8501 \\ --name streamlit-app \\ --env GOOGLE_APPLICATION_CREDENTIALS = /var/secret/cloud.google.com/gcp-service-account.json \\ -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json \\ -v $PWD /models:/home/aisg/from-gcs \\ asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /streamlit:0.1.0 Windows PowerShell $ docker run - -rm -p 8501 : 8501 ` - -name streamlit-app ` - -env GOOGLE_APPLICATION_CREDENTIALS =/ var / secret / cloud . google . com / gcp-service-account . json ` -v \"<PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json\" ` -v \" $( Get-Location ) \\models:/home/aisg/from-gcs\" ` asia . gcr . io /{{ cookiecutter . gcp_project_id }}/{{ cookiecutter . author_name }}/ streamlit : 0 . 1 . 0 {% endif %} - GOOGLE_APPLICATION_CREDENTIALS allows the container's entrypoint to download the predictive model specified during build time from GCS. - -v <PATH_TO_SA_JSON_FILE>:/var/secret/cloud.google.com/gcp-service-account.json attaches the JSON file for the service account credentials to the Docker container. - -v $PWD/models:/home/aisg/from-gcs allows the models downloaded to the host machine to be used by the container after being mounted to /home/aisg/from-gcs . To stop the container: $ docker container stop streamlit-app","title":"Docker Container"},{"location":"guide-for-user/12-streamlit/#integration-with-polyaxon","text":"Attention As this mode of deployment would take up resources in a long-running manner, please tear the service down through the dashboard once you've gone through this part of the guide. From the Docker build section, push the Docker image to GCR: {% if cookiecutter.gcr_personal_subdir == 'No' %} $ docker push asia.gcr.io/ {{ cookiecutter.gcp_project_id }} /streamlit:0.1.0 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} $ docker push asia.gcr.io/ {{ cookiecutter.gcp_project_id }} / {{ cookiecutter.author_name }} /streamlit:0.1.0 {% endif %} Then, push the configurations to the Polyaxon server to start up the Streamlit dashboard: Linux/macOS $ polyaxon run \\ -f aisg-context/polyaxon/polyaxonfiles/streamlit-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/streamlit:0.1.0\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run ` -f aisg-context / polyaxon / polyaxonfiles / streamlit-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/streamlit:0.1.0\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Linux/macOS $ polyaxon run \\ -f aisg-context/polyaxon/polyaxonfiles/streamlit-service.yml \\ -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/streamlit:0.1.0\" \\ -p {{ cookiecutter.repo_name }} -<YOUR_NAME> Windows PowerShell $ polyaxon run ` -f aisg-context / polyaxon / polyaxonfiles / streamlit-service . yml ` -P DOCKER_IMAGE = \"asia.gcr.io/{{cookiecutter.gcp_project_id}}/{{cookiecutter.author_name}}/streamlit:0.1.0\" ` -p {{ cookiecutter . repo_name }}-< YOUR_NAME > {% endif %} Just like with the VSCode or JupyterLab services, you can access the Streamlit service you've just spun up through the Polyaxon dashboard: Reference(s): Polyaxon - Integrations","title":"Integration with Polyaxon"},{"location":"guide-for-user/12-streamlit/#native-kubernetes-deployment-gke","text":"Attention As this mode of deployment would take up resources in a long-running manner, please tear it down once you've gone through this part of the guide. If you do not have the right permissions, please request assistance from your team lead or the administrators. Similar to deploying the FastAPI server , to deploy the Streamlit dashboard on GKE, you can make use of the sample Kubernetes manifest files provided with this template: $ kubectl apply -f aisg-context/k8s/dashboard/streamlit-deployment.yml --namespace = polyaxon-v1 $ kubectl apply -f aisg-context/k8s/dashboard/streamlit-service.yml --namespace = polyaxon-v1 To access the server, you can port-forward the service to a local port like such: Local Machine $ kubectl port-forward service/streamlit-svc 8501 :8501 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8501 -> 8501 Forwarding from [ ::1 ] :8501 -> 8501 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl port-forward service/streamlit- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc 8501 :8501 --namespace = polyaxon-v1 Forwarding from 127 .0.0.1:8501 -> 8501 Forwarding from [ ::1 ] :8501 -> 8501 {% endif %} Attention Please tear down the deployment and service objects once they are not required. Local Machine $ kubectl delete streamlit-deployment --namespace = polyaxon-v1 $ kubectl delete streamlit-svc --namespace = polyaxon-v1 {% elif cookiecutter.gcr_personal_subdir == 'Yes' %} Local Machine $ kubectl delete streamlit- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -deployment --namespace = polyaxon-v1 $ kubectl delete streamlit- {{ cookiecutter.author_name.replace ( '_' , '-' )}} -svc --namespace = polyaxon-v1 {% endif %} If you do not have the right permissions, please request assistance from your team lead or the administrators.","title":"Native Kubernetes Deployment (GKE)"}]}